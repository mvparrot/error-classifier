---
title: "Winners and Errors Classification"
author: "Dean"
date: "24 July 2017"
output: html_document
---

```{r setup, include=FALSE}

knitr::opts_chunk$set(echo    = TRUE,
                      warning = FALSE, 
                      message = FALSE, 
                      error   = FALSE,
                      cache   = FALSE)

options(scipen = 999)

```



## Loading in Data Set

```{r load_data}

setwd("~/2017 Work/Federer/erroR/data")
load("~/2017 Work/Federer/erroR/data/federer2017.RData")

df <- federer2017



library(ggplot2)
library(tidyr)
library(gridExtra)
library(GGally)
library(e1071)
library(MASS)
library(dplyr)
library(randomForest)
library(gbm)
library(rpart)
library(rpart.plot)
library(penalizedLDA)
library(xgboost)
library(foreign)
library(stringr)
library(stringi)
library(devtools)
library(fpp2)
library(neuralnet)
library(caret)
library(PPtreeViz)
library(knitr)
library(ggthemes)
library(Rtsne)
library(shiny)
library(tourr)
library(tidyverse)
library(plotly)


```


## Data Manipulation

The data has some issues such as NAs, so these had to be filtered out before I ran any models. In addition, there are many variables that I created in order to enhance the variables going into the models. There are variables that relate to the angle of the shot, the opponent's shot that comes to the player and the attributes of that shot, such as placement and speed, etc.  

```{r adding_variables1}

### Removing NA Observations
df <- df %>% 
      filter(is.na(unforced) == FALSE) %>% 
      filter(is.na(ballmark.x) == FALSE) %>% 
      filter(is.na(net.clearance) == FALSE) %>%
      filter(is.na(hitpoint) == FALSE)






for (i in 1:nrow(df)) {
  
  if (df$serve.classification[i] == 0) {
    
    df$serve.classification[i] <- "Ace"
    
  } else if (df$serve.classification[i] == 1) {
    
    df$serve.classification[i] <- "ReturnInPlay"
    
  } else if (df$serve.classification[i] == 2) {
    
    df$serve.classification[i] <- "ReturnOutOfPlay"
    
  } else if (df$serve.classification[i] == 3) {
    
    df$serve.classification[i] <- "Fault"
    
  }
  
  
  
  
  
  
  
  
  
  if (federer2017$serve.classification[i] == 0) {
    
    federer2017$serve.classification[i] <- "Ace"
    
  } else if (federer2017$serve.classification[i] == 1) {
    
    federer2017$serve.classification[i] <- "ReturnInPlay"
    
  } else if (federer2017$serve.classification[i] == 2) {
    
    federer2017$serve.classification[i] <- "ReturnOutOfPlay"
    
  } else if (federer2017$serve.classification[i] == 3) {
    
    federer2017$serve.classification[i] <- "Fault"
    
  }
  

  
}







df$end_of_point          <- c(rep(NA, nrow(df)-1), 1)



for (i in 1:(nrow(df) - 1)) {
  
  if (df$id[i] != df$id[i+1]) {
    
    
    df$end_of_point[i] <- 1
    
    
  } else {
    
    
    df$end_of_point[i] <- 0
    
    
  }
  
  
  
}








df$ballmark_x_ballmark_y <- df$ballmark.x * df$ballmark.y
df$winner                <- as.factor(df$winner)
df$serve.classification  <- as.factor(df$serve.classification)
df$side                  <- as.factor(df$side)
df$hitpoint              <- as.factor(df$hitpoint)


df <- df %>% dplyr::select(-matchid, -serveid)


```





```{r adding_variables2}


#Adding Vars

df <- df %>%
  mutate(isserver = ifelse(server == impact.player,1,0),
         retser   = ifelse(shot == 2, 1, 0),
         retser1  = ifelse(shot == 2 & serve.classification == 1, 1, 0),
         retser2  = ifelse(shot == 2 & serve.classification == 2, 1, 0))




for (i in 1:ncol(df)) {
  
  if(is.character(df[,i]) == TRUE) {
    
    df[,i] <- as.factor(df[,i])
    
  }
  
}




ids       <- df$id
split.ids <- t(data.frame(stri_split_boundaries(ids, type="character")))



df <- df %>%
      plyr::mutate(server.points   = as.numeric(split.ids[,1]), 
                   receiver.points = as.numeric(split.ids[,2]), 
                   server.games    = as.numeric(split.ids[,3]), 
                   receiver.games  = as.numeric(split.ids[,4]), 
                   server.sets     = as.numeric(split.ids[,5]), 
                   receiver.sets   = as.numeric(split.ids[,6]))




df <- df %>%
    mutate(server.sets.diff   = server.sets   - receiver.sets,
           server.games.diff  = server.games  - receiver.games,
           server.points.diff = server.points - receiver.points,
           speed.diff = NA, 
           oppo.hit.x = NA,
           oppo.hit.y = NA,
           oppo.hit.z = NA,
           oppo.speed = NA,
           oppo.ballmark.x = NA,
           oppo.ballmark.y = NA,
           oppo_ballmark_x_ballmark_y = NA,
           speed.ratio = NA)





for (i in 2:nrow(df)) {
  
  if(df$shot[i] != 1) {             #so that only non-serves are affected

      df$speed.diff[i] = df$speed1[i] - df$speed1[i-1]  #speed difference
      
      df$oppo.hit.x[i]      <- df$start.x[i-1]
      df$oppo.hit.y[i]      <- df$start.y[i-1]
      df$oppo.hit.z[i]      <- df$start.z[i-1]
      df$oppo.speed[i]      <- df$speed1[i-1]
      df$oppo.ballmark.x[i] <- df$ballmark.x[i-1]
      df$oppo.ballmark.y[i] <- df$ballmark.y[i-1]
      
      df$oppo_ballmark_x_ballmark_y[i] <- df$oppo.ballmark.x[i] * df$oppo.ballmark.y[i]  #oppo hit
  
  }
  
}





df <- df %>% 
  mutate(speed.ratio = speed1/oppo.speed) %>%             #speed ratio
  mutate(side.dist   = 4.115 - abs(oppo.ballmark.y)) %>%  #distance of oppo.ballmark from sideline
  mutate(base.dist   = 11.89 - abs(oppo.ballmark.x)) %>%  #distance of oppo.ballmark from baseline
  mutate(short.dist  = pmin(side.dist, base.dist)) %>%    #shortest distance from any line
  mutate(p.start.x   = NA) %>%
  mutate(p.start.y   = NA) %>%
  mutate(o.angle     = NA)




for (i in 3:nrow(df)) {
  
  if(df$impact.player[i] == df$impact.player[i-2] & df$shot[i] == df$shot[i-2] + 2) {
    
    df$p.start.x[i]=df$start.x[i-2]
    df$p.start.y[i]=df$start.y[i-2]
    
  }
  
}




# adding angle between fed.shot-opp.shot vector and opp.shot-opp.ballmark vector
# doing it in one line because df doesn't want to add vectors

  
for (i in 1:nrow(df)) {
  
  x1 <- df$p.start.x[i]
  y1 <- df$p.start.y[i]
  x2 <- df$oppo.hit.x[i]
  y2 <- df$oppo.hit.y[i]
  x3 <- df$oppo.ballmark.x[i]
  y3 <- df$oppo.ballmark.y[i]
  
  o.angle <- acos(
  ((c(x1,y1)-c(x2,y2))/sqrt((x1-x2)^2+(y1-y2)^2)) %*%
    ((c(x2,y2)-c(x3,y3))/sqrt((x2-x3)^2+(y2-y3)^2))) * 180/pi
  
  df$o.angle[i] = ifelse(o.angle > 90, 180-o.angle, o.angle)
  
}


#Now adding the angle the player hits
df <- df %>%
  mutate(p.angle = NA)
  
for (i in 1:nrow(df)) {
  
  x1 <- df$oppo.hit.x[i]
  y1 <- df$oppo.hit.y[i]
  x2 <- df$start.x[i]
  y2 <- df$start.y[i]
  x3 <- df$ballmark.x[i]
  y3 <- df$ballmark.y[i]
  
  p.angle <- acos(
  ((c(x1,y1)-c(x2,y2))/sqrt((x1-x2)^2+(y1-y2)^2)) %*%
    ((c(x2,y2)-c(x3,y3))/sqrt((x2-x3)^2+(y2-y3)^2))) * 180/pi
  
  df$p.angle[i] = ifelse(p.angle > 90, 180-p.angle, p.angle)
  
}



```








```{r}

# df_long <- df %>%
#             filter(end_of_point == 1) %>%
#             gather(key = vars, value = value, x01:z32)

```



```{r}

# ggplot(df_long) +
#   geom_boxplot(aes(x = as.factor(winner), y = value, fill = as.factor(winner))) +
#   facet_wrap(~vars, scale = "free_y")




```






## Filtering Data Set

In order to determine whether the last shot of the point was a winner or error, I filtered the data so that we are only looking at the last point of the rally. 

```{r}

df <- df %>% filter(end_of_point == 1) %>% filter(hitpoint != "S")



df <- df %>% dplyr::select(-server.score,
                          -receiver.score,
                          -server.won,
                          -impact.player,
                          -server,
                          -receiver, 
                          -serve.classification,
                          -start2,
                          -end2, 
                          -forced, 
                          -unforced,
                          -end_of_point,
                          -p.start.x,
                          -p.start.y,
                          -o.angle,
                          -id)

```









# Classification Models
## Training and Test

The data was split into 2/3rds of training data and 1/3 of test data so that all the models could be tested properly. 

```{r training_and_test}

set.seed(1000)
idx <- sample(nrow(df), nrow(df) * (2/3)) 

tr  <- df[idx,]
ts  <- df[-idx,]



ts2 <- federer2017[-idx,]





```












### Random Forest and Variable Selection Based Off Accuracy Measures

The Random Forest model was created first due to the additional features of the function that allow the user to determine the importance of the variables. This was ideal as there were a lot of variables used in the model and it was hard to determine whether a particular variable was significant or not.

```{r random_forest}

tr_rf   <- tr %>% 
            dplyr::select(-winner) %>% 
            randomForest(tr$winner,
                          ntree=1000, 
                          importance = TRUE)

plot(tr_rf, main = "Random Forest Model")
legend(650, 0.7, c("Winner Error Rate", "OOB Error Rate", "Non-Winner Error Rate"), fill = c(3,1,2))


```

We can see from the plot of the Random Forest error rate as a function of the number of trees that the Out-of-Bag error is roughly 35% for the majority of values, so in reality there is no need to fit 1000 trees. This value plateaus after roughly 200 trees, which is quite rapid. The next step is to predict the test set values using the generated Random Forest model.


```{r}

pred_rf <- predict(tr_rf, 
                   ts %>% dplyr::select(-winner), 
                   type="class") #Prediction using test data

df_rf <- data.frame(Actual = ts$winner, 
                    Predicted = pred_rf,
                    stringsAsFactors = FALSE)

(rf_table1 <- table(Actual = ts$winner, Predicted = pred_rf))




```


Based off the Out-of-Bag sample, the estimated error rate on the test set was deemed to be 35.12%. The actual error rate on the test set ended up being `r paste((rf_table1[1,2] + rf_table1[2,1] / sum(rf_table1)) * 100, "%", sep = "")`. However, this Random Forest was conducted on the entire set of variables, so the next step would be to filter out potential noise variables and then refit the Random Forest model. I created a Variable Importance Plot in order to identify any significant drops in variable importance, so that I could build a model solely using the signficiant variables. This is similar to a Scree Plot in Principal Component Analysis.

```{r}

rf_imp <- data.frame(Var = rownames(tr_rf$importance), tr_rf$importance, stringsAsFactors = FALSE) %>%
                     arrange(desc(MeanDecreaseAccuracy))

varImpPlot(tr_rf, sort = TRUE)

```

From here we can see that the variable "start.z" was generally the most important variable in the model, together with the "start.x" position. The variable "start.z" indicates the height of the ball when hit and "start.x" indicates the depth of the ball when hit. This makes sense as the majority of smashes are at a increased height to an average shot, and a smash is more likely to be a winner than an ordinary shot. This is the same with the x position. If the player hitting the ball hits it from the baseline, he/she is less likely to hit a winner than if the ball has dropped short and closer to the net. From here, I filtered the variables based off a fairly arbitrary criterion. The variables that I kept were the ones that either had a "MeanDecreaseGini" of greater than 4.5 or a "MeanDecreaseAccuracy" of greater than 0.0003.

```{r filtering_columns_using_randomForest}


### Filtering the Columns Based Off Importance ###

rf_imp$dummy <- ifelse(rf_imp$MeanDecreaseGini > 4.5 | rf_imp$MeanDecreaseAccuracy > 0.0003, 1, 0)
rf_imp       <- rf_imp[which(rf_imp$dummy == 1),]
rf_var_list  <- rf_imp$Var
rf_var_list  <- append(rf_var_list, "winner")



### Keeping only the variables in "rf_var_list"

tr <- tr[, which(colnames(tr) %in% rf_var_list)]
ts <- ts[, which(colnames(ts) %in% rf_var_list)]

```









### Random Forest With Most Important Variables

Once the less important variables were filtered out, I refit the Random Forest model once again, hoping that the model would perform better. 

```{r random_forest2}

tr_rf   <- tr %>% 
            dplyr::select(-winner) %>% 
            randomForest(tr$winner,
                          ntree=1000, 
                          importance = TRUE)

plot(tr_rf, main = "Random Forest Model")
legend(650, 0.7, c("Winner Error Rate", "OOB Error Rate", "Non-Winner Error Rate"), fill = c(3,1,2))


```






```{r}

pred_rf <- predict(tr_rf, 
                   ts %>% dplyr::select(-winner), 
                   type="class") #Prediction using test data



df_rf <- data.frame(Actual = ts$winner, 
                    Predicted = pred_rf,
                    stringsAsFactors = FALSE)



(rf_table <- table(Actual = ts$winner, Predicted = pred_rf))

```

The classifier slightly improved from earlier, with `r rf_table[1,2] + rf_table[2,1]` misclassifications compared to `r rf_table1[1,2] + rf_table1[2,1]` previously. In this case, the classifier predicted an Out-of-Bag error rate of 33.78%, compared to 35.12% in the first fit of the Random Forest. There were four errors in the first fit of the model that were now correctly classified as winners, which was encouraging. 





## SVM

The next idea was to use Support Vector Machines, which find the boundary which gives the best separation between classes. 

### SVM Radial

The first method used was the "Radial" Support Vector Machine kernel. This method gives flexible boundaries, rather than just being a constant linear hyperplane. 

```{r svm_radial}

tr_svm_radial   <- svm(winner ~ ., 
                      data = tr, 
                      kernel = "radial")


pred_svm_radial <- predict(tr_svm_radial, ts)          
df_svm_radial   <- data.frame(Actual = ts$winner, 
                              Predicted = pred_svm_radial) 


(svm_radial_table <- table(ts$winner, pred_svm_radial))

```

As we can see, the Radial SVM had `r svm_radial_table[1,2] + svm_radial_table[2,1]` misclassifications, meaning an error rate of `r paste(round(((svm_radial_table[1,2] + svm_radial_table[2,1]) * 100) / sum(svm_radial_table), 2), "%", sep = "")`. The model correctly predicted `r svm_radial_table[2,2]` winners out of the 111 in the test set. 









### SVM Linear

The next Support Vector Machine kernel I used was the linear kernel. It provides the hyperplane with the best separation between classes. 

```{r svm_linear}

tr_svm_linear   <- svm(winner ~ ., 
                      data = tr, 
                      kernel = "linear")


pred_svm_linear <- predict(tr_svm_linear, ts)          
df_svm_linear   <- data.frame(Actual = ts$winner, 
                              Predicted = pred_svm_linear) 


(svm_linear_table <- table(ts$winner, pred_svm_linear))

```

This model correctly predicted `r svm_linear_table[2,2]` winners and had a total of `r svm_linear_table[1,2] + svm_linear_table[2,1]` misclassifications. 










### SVM Polynomial

The final Support Vector Machine method used was a polynomial kernel with a degree of 2 (quadratic). 

```{r svm_polynomial}

tr_svm_polynomial   <- svm(winner ~ ., 
                      data = tr, 
                      kernel = "polynomial",
                      degree = 2)


pred_svm_polynomial <- predict(tr_svm_polynomial, ts)          
df_svm_polynomial   <- data.frame(Actual = ts$winner, 
                                  Predicted = pred_svm_polynomial) 


(svm_polynomial_table <- table(ts$winner, pred_svm_polynomial))

```

This method had `r svm_polynomial_table[1,2] + svm_polynomial_table[2,1]` misclassifications and `r svm_polynomial_table[2,2]` correctly predicted winners. 


















### Decision Tree 

```{r decision_tree}

tr_decisiontree <- rpart(winner ~ ., 
                         data=tr,
                         method="class")         

prp(tr_decisiontree)     


```

The decision tree only uses 6 variables: "start.z", "x32", "oppo.speed", "oppo.ballmark.y", "oppo_ballmark_x_ballmark_y" and "base.dist". It should be noted that "x32" is the fourth coefficient in the 3-degree polynomial describing the last arc of the shot for the x-dimension. What is interesting to note is that the opponent's shot components show up in this model, meaning the placement of the opponent's shot and the speed of the opponent's shot are crucial in determining whether a shot will be an ace. While this is intuitive, it was encouraging to see this show up in the data. 



```{r}

pred_decisiontree <- predict(tr_decisiontree,              
                             ts, 
                             type="class")


df_decisiontree   <- data.frame(Actual    = ts$winner, 
                                Predicted = pred_decisiontree,
                                stringsAsFactors = FALSE) 


(decision_tree_table <- table(Actual    = ts$winner, 
                              Predicted = pred_decisiontree))

```

The decision tree had a total of `r decision_tree_table[1,2] + decision_tree_table[2,1]` misclassifications and correctly predicted `r decision_tree_table[2,2]` winners out of the 111. 












### XGBoost

Another model type that I used was a Gradient Boosting model, which is conducted through the R library "xgboost". I set some of the parameters based off a trial and error methodology, so that's where the various values came from. 

```{r xgboost}


cls <- ifelse(tr[,"winner"] == 1, 2, 1)

dtrain <- tr %>% 
          dplyr::select(-winner) %>% 
          as.matrix %>% 
          xgb.DMatrix(label=cls)

param  <- list(max.depth = 8, eta = 0.5, silent = 1)
tr_xgb <- xgb.train(param, dtrain, nthread = 5, nround = 20)


ts_matrix <- ts %>% 
              dplyr::select(-winner) %>% 
              as.matrix

pxgb <- round(predict(tr_xgb, ts_matrix), 0)

pxgb <- ifelse(pxgb == 2, 1, 0)
pxgb <- as.factor(pxgb)




(xgboost_table <- table(Actual = ts$winner, Predicted = pxgb))

df_xgboost   <- data.frame(Actual    = ts$winner, 
                           Predicted = pxgb,
                           stringsAsFactors = FALSE) 



```

Overall, the Gradient Boosting algorithm performed fairly well. It correctly predicted `r xgboost_table[2,2]` winners but had a total of `r xgboost_table[1,2] + xgboost_table[2,1]` misclassifications, which was quite high. 







### Logit

The next model type is a Logistic Regression model, which is a form of regression for probabilities. The response variable is binary, with a 1 representing a winner and a 0 representing everything else. Due to the large number of variables in the training set, I also used a backwards stepwise regression, so that the model selected had a lower AIC value than the model with all variables. 

```{r logit}

tr_logit   <- glm(winner ~ ., family = binomial(link="logit"), data = tr)


tr_logit   <- suppressWarnings(step(tr_logit, direction = "backward", trace = FALSE))
tr_logit %>% summary


pred_logit <- predict(tr_logit, ts, type="response")
pred_logit <- ifelse(pred_logit >= 0.5, 1, 0)

df_logit   <- data.frame(Actual    = ts$winner, 
                         Predicted = pred_logit,
                         stringsAsFactors = FALSE) 


(logit_table <- table(Actual    = ts$winner, 
                      Predicted = pred_logit))



```

The Logistic Regression model has performed solidly, with an overall number of misclassifications of `r logit_table[1, 2] + logit_table[2, 1]` and with `r logit_table[2,2]` correctly predicted winners. 













### Probit

The probit model is another form of binary probability regression, with the response variable being a 1 if the shot was a winner and 0 for all other shots. 

```{r probit}
tr_probit   <- glm(winner ~ ., family = binomial(link="probit"), data = tr)


tr_probit   <- suppressWarnings(step(tr_probit, trace = FALSE))
tr_probit %>% summary


pred_probit <- predict(tr_probit, ts, type="response")
pred_probit <- ifelse(pred_probit >= 0.5, 1, 0)

df_probit   <- data.frame(Actual    = ts$winner, 
                          Predicted = pred_probit,
                          stringsAsFactors = FALSE) 


(probit_table  <- table(Actual    = ts$winner, 
                        Predicted = pred_probit))



```

The Probit model didn't perform as strongly as the Logistic Regression model, with the number of misclassifications being `r probit_table[1, 2] + probit_table[2, 1]` and the number of correctly predicted winners being `r probit_table[2, 2]`. 














### All Predictions Combined

In order to compare the results of each classification method, I've created a data frame of all predictions side by side called "comparison". In addition, I created the data frame "error_count" to show the total misclassifications for each class along with the number of correctly predicted aces, so the methods can be directly compared to one another. 

```{r comparison}
comparison <- data.frame(Actual            = ts$winner,
                         'Random Forest'   = df_rf$Predicted,
                         'SVM Radial'      = df_svm_radial$Predicted,
                         'SVM Linear'      = df_svm_linear$Predicted,
                         'SVM Polynomial'  = df_svm_polynomial$Predicted,
                         'Decision Tree'   = df_decisiontree$Predicted,
                         XGBoost           = df_xgboost$Predicted,
                         Logit             = df_logit$Predicted,
                         Probit            = df_probit$Predicted, stringsAsFactors = FALSE)




error_count <- data.frame('Random Forest'  = c(sum(comparison$Random.Forest  != comparison$Actual), rf_table[2, 2]),
                          'SVM Radial'     = c(sum(comparison$SVM.Radial     != comparison$Actual), svm_radial_table[2, 2]), 
                          'SVM Linear'     = c(sum(comparison$SVM.Linear     != comparison$Actual), svm_linear_table[2, 2]),
                          'SVM Polynomial' = c(sum(comparison$SVM.Polynomial != comparison$Actual), svm_polynomial_table[2, 2]),
                          'Decision Tree'  = c(sum(comparison$Decision.Tree  != comparison$Actual), decision_tree_table[2, 2]),
                          XGBoost          = c(sum(comparison$XGBoost        != comparison$Actual), xgboost_table[2, 2]),
                          Logit            = c(sum(comparison$Logit          != comparison$Actual), logit_table[2, 2]),
                          Probit           = c(sum(comparison$Probit         != comparison$Actual), probit_table[2, 2]), 
                          stringsAsFactors = FALSE)


rownames(error_count) <- c("Total Misclassifications", "Correct Winner Predictions")



error_count$Best <- c(ifelse(min(error_count[1,]) == error_count$Random.Forest[1],  "Random Forest",
                      ifelse(min(error_count[1,]) == error_count$SVM.Radial[1],     "SVM Radial",
                      ifelse(min(error_count[1,]) == error_count$SVM.Linear[1],     "SVM Linear",
                      ifelse(min(error_count[1,]) == error_count$SVM.Polynomial[1], "SVM Polynomial",
                      ifelse(min(error_count[1,]) == error_count$Decision.Tree[1],  "Decision Tree",
                      ifelse(min(error_count[1,]) == error_count$XGBoost[1],        "XGBoost", "Logit")))))),
                    
                      ifelse(max(error_count[2,]) == error_count$Random.Forest[2],  "Random Forest",
                      ifelse(max(error_count[2,]) == error_count$SVM.Radial[2],     "SVM Radial",
                      ifelse(max(error_count[2,]) == error_count$SVM.Linear[2],     "SVM Linear",
                      ifelse(max(error_count[2,]) == error_count$SVM.Polynomial[2], "SVM Polynomial",
                      ifelse(max(error_count[2,]) == error_count$Decision.Tree[2],  "Decision Tree",       
                      ifelse(max(error_count[2,]) == error_count$XGBoost[2],        "XGBoost", "Logit")))))))


kable(error_count, caption = "Model Performance", align = c("c","c","c","c","c","c","c","c","c","c"))
                         
```

As we can see, the model with the least amount of misclassifications was the `r error_count$Best[1]` model with `r min(error_count[1, 1:(ncol(error_count)-1)])` misclassifications out of the `r nrow(ts)` observations in the test set. This resulted in an error rate of `r paste(round((min(error_count[1, 1:(ncol(error_count)-1)]) * 100)/nrow(ts), 2), "%", sep = "")`.

After seeing these results, I decided to identify the reasons why the models had a fairly poor error rate. This is conducted in the next section. 






# Improvements to the Models

Part of the diagnosis of the model performance involved looking at a subset of observations that were either clearly predicted to be winners or clearly predicted to not be a winner. This was done in order to identify the variables that provided the best separation, so that a second version of classification algorithms could be conducted using a specific subset of the variables remaining in the training set. 

## Identifying Patterns in Clear Winners vs. Unpredictable Winners

I created a subset data frame of the test set that included observations that were winners and had at least 5 models predict a winner, or observations that were not winners and had no models predicting a winner. This data frame was called "identifying_trends". 

```{r}

identifying_trends <- cbind(ts,
                        comparison)
                        

identifying_trends$Pred_RF             <- ifelse(identifying_trends$Random.Forest == "0", 0, 1)
identifying_trends$Pred_SVM_Radial     <- ifelse(identifying_trends$SVM.Radial == "0", 0, 1)
identifying_trends$Pred_SVM_Linear     <- ifelse(identifying_trends$SVM.Linear == "0", 0, 1)
identifying_trends$Pred_SVM_Polynomial <- ifelse(identifying_trends$SVM.Polynomial == "0", 0, 1)
identifying_trends$Pred_XGBoost        <- ifelse(identifying_trends$XGBoost == "0", 0, 1)


identifying_trends <- identifying_trends %>% 
                        dplyr::select(-Random.Forest, -SVM.Radial, -SVM.Linear, -SVM.Polynomial, -XGBoost)

identifying_trends$countif   <- NA
identifying_trends$Dummy     <- NA




for (i in 1:nrow(identifying_trends)) {
  
  identifying_trends$countif[i] <- sum(identifying_trends[i, (ncol(identifying_trends)-8):(ncol(identifying_trends)-2)])
  
  if ((identifying_trends$countif[i] >= 5 && identifying_trends$Actual[i] == 1) || 
      (identifying_trends$countif[i] == 0 && identifying_trends$Actual[i] == 0)) {
    
    identifying_trends$Dummy[i] <- 1
    
  } else {
    
    identifying_trends$Dummy[i] <- 0
    
  }
  
}



identifying_trends <- identifying_trends %>% 
                        filter(Dummy == 1)


```


From here, I created various long format data frames from the "identifying_trends" data frame and inspected each variable one by one. 

```{r}

ident_trends_long1 <- identifying_trends %>% gather(key = variable, value = stat, seconds:z32)


ggplot(data = ident_trends_long1, aes(x = winner, y = stat)) + 
  geom_boxplot(aes(fill = winner)) + 
  facet_wrap(~variable, scales = "free")






ident_trends_long2 <- identifying_trends %>% gather(key = variable, value = stat, start:importance)


ggplot(data = ident_trends_long2, aes(x = winner, y = stat)) + 
  geom_boxplot(aes(fill = winner)) + 
  facet_wrap(~variable, scales = "free")







ident_trends_long3 <- identifying_trends %>% gather(key = variable, value = stat, ballmark_x_ballmark_y:p.angle)


ggplot(data = ident_trends_long3, aes(x = winner, y = stat)) + 
  geom_boxplot(aes(fill = winner)) + 
  facet_wrap(~variable, scales = "free")




```

Based off these various boxplots, I deemed that the following variables were the variables with the best separation:

* oppo.hit.y
* oppo.hit.z
* oppo.speed
* oppo.ballmark.y
* oppo_ballmark_x_ballmark_y
* short.dist
* speed.diff
* speed.ratio
* ballmark.y
* speed1
* speed2
* start.z
* end
* final.y

As a result, I decided to refit all classification models with this subset of variables.


# Final Classification Models

## Using the Stated Subset of Variables

```{r}

var_list <- c("oppo.hit.y", "oppo.hit.z", "oppo.speed", "oppo.ballmark.y", "oppo_ballmark_x_ballmark_y",
              "short.dist", "speed.diff", "speed.ratio", "ballmark.y", "speed1", "speed2", "start.z", 
              "end", "final.y", "winner")



tr3 <- tr[, which(colnames(tr) %in% var_list)]
ts3 <- ts[, which(colnames(ts) %in% var_list)]


```

After specifying the subset of variables to use, there were only `r ncol(tr3)-1` remaining for the models. 





### Random Forest With Most Important Variables

Once the less important variables were filtered out, I refit the Random Forest model once again. 

```{r random_forest3}

tr_rf2   <- tr3 %>% 
            dplyr::select(-winner) %>% 
            randomForest(tr3$winner,
                          ntree=1000, 
                          importance = TRUE)


plot(tr_rf2, main = "Random Forest Model")
legend(650, 0.7, c("Winner Error Rate", "OOB Error Rate", "Non-Winner Error Rate"), fill = c(3,1,2))


```

Similar to the previous version of the Random Forest model, the OOB Error Rate was roughly 35% and this plateaued after only 400 trees. 

```{r}

pred_rf2 <- predict(tr_rf2, 
                   ts3 %>% dplyr::select(-winner), 
                   type="class") #Prediction using test data



df_rf2 <- data.frame(Actual = ts3$winner, 
                    Predicted = pred_rf2,
                    stringsAsFactors = FALSE)


rf_imp2 <- data.frame(Var = rownames(tr_rf2$importance), tr_rf2$importance, stringsAsFactors = FALSE) %>%
          arrange(desc(MeanDecreaseAccuracy))


varImpPlot(tr_rf2, sort = TRUE)

```

With this subset of variables, the Variable Importance Plot still shows the variable "start.z" as being the most important, based off the "Mean Decrease Gini" index. 


```{r}

(rf_table2 <- table(Actual = ts3$winner, Predicted = pred_rf2))

```

In this case, the classifier predicted an Out-of-Bag error rate of 34.23%, compared to 33.78% in the first fit of the Random Forest. The number of correctly predicted winners increased from `r error_count$Random.Forest[2]` to `r rf_table2[2,2]`, but the number of misclassifications increased from `r error_count$Random.Forest[1]` to `r rf_table2[1,2] + rf_table[2,1]`.





## SVM

### SVM Radial

```{r svm_radial2}

tr_svm_radial2   <- svm(winner ~ ., 
                      data = tr3, 
                      kernel = "radial")


pred_svm_radial2 <- predict(tr_svm_radial2, ts3)          
df_svm_radial2   <- data.frame(Actual = ts3$winner, 
                              Predicted = pred_svm_radial2) 


(svm_radial_table2 <- table(Actual = ts3$winner, Predicted = pred_svm_radial2))

```

The SVM Radial model had the more misclassifications than before, with `r svm_radial_table2[2,1] + svm_radial_table2[1,2]` misclassifications now compared to `r error_count$SVM.Radial[1]`. However, the new version correctly predicted `r svm_radial_table2[2, 2]` winners compared to `r svm_radial_table[2,2]` winners previously.








### SVM Linear

```{r svm_linear2}

tr_svm_linear2   <- svm(winner ~ ., 
                      data = tr3, 
                      kernel = "linear")


pred_svm_linear2 <- predict(tr_svm_linear2, ts3)          
df_svm_linear2   <- data.frame(Actual = ts3$winner, 
                              Predicted = pred_svm_linear2) 


(svm_linear_table2 <- table(Actual = ts3$winner, Predicted = pred_svm_linear2))

```

The SVM Linear model had only `r svm_linear_table2[1,2] + svm_linear_table2[2,1]` misclassifications, but didn't predict one winner, so it is a fairly useless model in this situation. This was a significant decrease in performance compared to the earlier fit, which had only `r error_count$SVM.Linear[1]` misclassifications and correctly predicted `r error_count$SVM.Linear[2]` winners. 










### SVM Polynomial

```{r svm_polynomial2}

tr_svm_polynomial2   <- svm(winner ~ ., 
                       data = tr3, 
                       kernel = "polynomial",
                       degree = 2)


pred_svm_polynomial2 <- predict(tr_svm_polynomial2, ts3)          
df_svm_polynomial2   <- data.frame(Actual = ts3$winner, 
                                  Predicted = pred_svm_polynomial2) 


(svm_polynomial_table2 <- table(Actual = ts3$winner, Predicted = pred_svm_polynomial2))

```

The second version of the SVM Polynomial model improved slightly upon the first version. The total number of misclassifications was now `r svm_polynomial_table2[1,2] + svm_polynomial_table2[2,1]` compared to `r error_count$SVM.Polynomial[1]` misclassifications previously. In addition, the total number of correctly predicted winners was `r svm_polynomial_table2[2,2]` in the second version compared to `r error_count$SVM.Polynomial[2]` previously. 

















### Decision Tree 

```{r decision_tree2}

tr_decisiontree2 <- rpart(winner ~ ., 
                         data=tr3,
                         method="class")         

prp(tr_decisiontree2)     

```

This version of the decision tree used similar variables to the first version. This time the model included "speed.diff", which was the difference between the speed of the player's shot and the opponent's shot. 

```{r}

pred_decisiontree2 <- predict(tr_decisiontree2,              
                             ts3, 
                             type="class")


df_decisiontree2   <- data.frame(Actual    = ts3$winner, 
                                Predicted = pred_decisiontree2,
                                stringsAsFactors = FALSE) 


(decision_tree_table2 <- table(Actual    = ts3$winner, 
                              Predicted = pred_decisiontree2))


```

The decision tree performed worse in the second version, with a total of `r decision_tree_table2[2,2]` correctly predicted winners compared to `r error_count$Decision.Tree[2]` correctly predicted winners previously. In addition, the total number of misclassifications this time around was `r decision_tree_table2[1,2] + decision_tree_table2[2,1]` compared to `r error_count$Decision.Tree[1]` previously. 













### XGBoost

```{r xgboost2}


cls <- ifelse(tr3[,"winner"] == 1, 2, 1)

dtrain <- tr3 %>% 
          dplyr::select(-winner) %>% 
          as.matrix %>% 
          xgb.DMatrix(label=cls)

param <- list(max.depth = 8, eta = 0.5, silent = 1)
tr_xgb2 <- xgb.train(param, dtrain, nthread = 5, nround = 20)


ts_matrix2 <- ts3 %>% 
              dplyr::select(-winner) %>% 
              as.matrix

pxgb2 <- round(predict(tr_xgb2, ts_matrix2), 0)

pxgb2 <- ifelse(pxgb2 == 2, 1, 0)
pxgb2 <- as.factor(pxgb2)




(xgboost_table2 <- table(Actual = ts3$winner, Predicted = pxgb2))


df_xgboost2   <- data.frame(Actual    = ts3$winner, 
                           Predicted = pxgb2,
                           stringsAsFactors = FALSE) 



```

The Gradient Boosting model improved its number of correct winner predictions, with `r xgboost_table2[2,2]` in the second version compared to `r error_count$XGBoost[2]` in the first version. The total number of misclassifications increased from `r error_count$XGBoost[1]` in version 1 to `r xgboost_table2[1,2] + xgboost_table2[2,1]` misclassifications in version 2. 







### Logit


```{r logit2}

tr_logit2   <- glm(winner ~ ., family = binomial(link="logit"), data = tr3)


tr_logit2   <- suppressWarnings(step(tr_logit2, direction = "backward", trace = FALSE))
tr_logit2 %>% summary


pred_logit2 <- predict(tr_logit2, ts3, type="response")
pred_logit2 <- ifelse(pred_logit2 >= 0.5, 1, 0)

df_logit2   <- data.frame(Actual    = ts3$winner, 
                         Predicted = pred_logit2,
                         stringsAsFactors = FALSE) 


(logit_table2 <- table(Actual    = ts3$winner, 
                      Predicted = pred_logit2))



```

The Logit model performed worse in version 2, with `r logit_table2[2,2]` correctly predicted winners compared to `r error_count$Logit[2]` previously. In contrast, the second version model had a total number of `r logit_table2[1,2] + logit_table2[2,1]` misclassifications compared to `r error_count$Logit[1]` misclassifications previously. 








### Probit

```{r probit2}
tr_probit2   <- glm(winner ~ ., family = binomial(link="probit"), data = tr3)
tr_probit2 %>% summary

tr_probit2   <- suppressWarnings(step(tr_probit2, trace = FALSE))



pred_probit2 <- predict(tr_probit2, ts3, type="response")
pred_probit2 <- ifelse(pred_probit2 >= 0.5, 1, 0)

df_probit2   <- data.frame(Actual    = ts3$winner, 
                          Predicted = pred_probit2,
                          stringsAsFactors = FALSE) 


(probit_table2  <- table(Actual    = ts3$winner, 
                        Predicted = pred_probit2))



```

The probit model also performed fairly poor, with only `r probit_table2[2,2]` correctly predicted winners, down from `r error_count$Probit[2]` in version 1. The improvement came in the misclassification rate, with `r probit_table2[1,2] + probit_table2[2,1]` misclassifications compared to `r error_count$Probit[1]` previously. 





















### All Predictions Combined

Just as I did earlier in the report, I've created a data frame of all predictions side by side called "comparison2". In addition, I created the data frame "error_count2" to show the total misclassifications for each class along with the number of correctly predicted winners, so the methods can be directly compared to one another. 


```{r comparison2}

comparison2 <- data.frame(Actual            = ts3$winner,
                          'Random Forest'   = df_rf2$Predicted,
                          'SVM Radial'      = df_svm_radial2$Predicted,
                          'SVM Linear'      = df_svm_linear2$Predicted,
                          'SVM Polynomial'  = df_svm_polynomial2$Predicted,
                          'Decision Tree'   = df_decisiontree2$Predicted,
                          XGBoost           = df_xgboost2$Predicted,    
                          Logit             = df_logit2$Predicted,
                          Probit            = df_probit2$Predicted, stringsAsFactors = FALSE)




error_count2 <- data.frame('Random Forest'  = c(sum(comparison2$Random.Forest  != comparison2$Actual), rf_table2[2, 2]),
                           'SVM Radial'     = c(sum(comparison2$SVM.Radial     != comparison2$Actual), svm_radial_table2[2, 2]), 
                           'SVM Linear'     = c(sum(comparison2$SVM.Linear     != comparison2$Actual), svm_linear_table2[2, 2]),
                           'SVM Polynomial' = c(sum(comparison2$SVM.Polynomial != comparison2$Actual), svm_polynomial_table2[2, 2]),
                           'Decision Tree'  = c(sum(comparison2$Decision.Tree  != comparison2$Actual), decision_tree_table2[2, 2]),
                           XGBoost          = c(sum(comparison2$XGBoost        != comparison2$Actual), xgboost_table2[2, 2]),
                           Logit            = c(sum(comparison2$Logit          != comparison2$Actual), logit_table2[2, 2]),
                           Probit           = c(sum(comparison2$Probit         != comparison2$Actual), probit_table2[2, 2]), 
                           stringsAsFactors = FALSE)


rownames(error_count2) <- c("Total Misclassifications", "Correct Winner Predictions")



error_count2$Best <- c(ifelse(min(error_count2[1,]) == error_count2$Random.Forest[1],  "Random Forest",
                       ifelse(min(error_count2[1,]) == error_count2$SVM.Radial[1],     "SVM Radial",
                       ifelse(min(error_count2[1,]) == error_count2$SVM.Linear[1],     "SVM Linear",
                       ifelse(min(error_count2[1,]) == error_count2$SVM.Polynomial[1], "SVM Polynomial",
                       ifelse(min(error_count2[1,]) == error_count2$Decision.Tree[1],  "Decision Tree",       
                       ifelse(min(error_count2[1,]) == error_count2$XGBoost[1],        "XGBoost", "Logit")))))),
                    
                       ifelse(max(error_count2[2,]) == error_count2$Random.Forest[2],  "Random Forest",
                       ifelse(max(error_count2[2,]) == error_count2$SVM.Radial[2],     "SVM Radial",
                       ifelse(max(error_count2[2,]) == error_count2$SVM.Linear[2],     "SVM Linear",
                       ifelse(max(error_count2[2,]) == error_count2$SVM.Polynomial[2], "SVM Polynomial",
                       ifelse(max(error_count2[2,]) == error_count2$Decision.Tree[2],  "Decision Tree",
                       ifelse(max(error_count2[2,]) == error_count2$XGBoost[2],        "XGBoost", "Logit")))))))



kable(error_count2, caption = "Model Performance", align = c("c","c","c","c","c","c","c","c","c"))

                         
```

Ultimately, the best performing model in terms of the total number of misclassifications was the SVM Linear model, with `r error_count2$SVM.Linear[1]` misclassifications. However, closer look at the results of this method shows that it didn't actually predict any winners, so all the misclassifications are from the 111 winners in the test sample. 

The best method in terms of number of correctly predicted winners was the XGBoost model, with `error_count2$XGBoost[2]` correctly predicted winners (out of the 111 in the test sample). 









## Comparison of Versions of Models

In order to determine which version of each model performed the best, it made sense to create a data frame that has the number of misclassifications for each version and the number of correctly predicted winners for each version and then compare the two scenarios. 

```{r}

all_comparisons <- rbind(error_count[,-ncol(error_count)], 
                         error_count2[,-ncol(error_count)])


new_row  <- data.frame(Random.Forest  = c(ifelse(all_comparisons$Random.Forest[1] < all_comparisons$Random.Forest[3], "V1", "V2"),
                                          ifelse(all_comparisons$Random.Forest[2] > all_comparisons$Random.Forest[4], "V1", "V2"),
                                          NA),
                       
                       SVM.Radial     = c(ifelse(all_comparisons$SVM.Radial[1] < all_comparisons$SVM.Radial[3], "V1", "V2"),
                                          ifelse(all_comparisons$SVM.Radial[2] > all_comparisons$SVM.Radial[4], "V1", "V2"),
                                          NA),
                       
                       SVM.Linear     = c(ifelse(all_comparisons$SVM.Linear[1] < all_comparisons$SVM.Linear[3], "V1", "V2"),
                                          ifelse(all_comparisons$SVM.Linear[2] > all_comparisons$SVM.Linear[4], "V1", "V2"),
                                          NA),
                       
                      SVM.Polynomial = c(ifelse(all_comparisons$SVM.Polynomial[1] < all_comparisons$SVM.Polynomial[3], "V1", "V2"),
                                         ifelse(all_comparisons$SVM.Polynomial[2] > all_comparisons$SVM.Polynomial[4], "V1", "V2"),
                                         NA),
                                          
                       Decision.Tree  = c(ifelse(all_comparisons$Decision.Tree[1] < all_comparisons$Decision.Tree[3], "V1", "V2"),
                                          ifelse(all_comparisons$Decision.Tree[2] > all_comparisons$Decision.Tree[4], "V1", "V2"),
                                          NA),
                       
                       XGBoost        = c(ifelse(all_comparisons$XGBoost[1] < all_comparisons$XGBoost[3], "V1", "V2"),
                                          ifelse(all_comparisons$XGBoost[2] > all_comparisons$XGBoost[4], "V1", "V2"),
                                          NA),
                                          
                       Logit          = c(ifelse(all_comparisons$Logit[1] < all_comparisons$Logit[3], "V1", "V2"),
                                          ifelse(all_comparisons$Logit[2] > all_comparisons$Logit[4], "V1", "V2"),
                                          NA),
                                          
                       Probit         = c(ifelse(all_comparisons$Probit[1] < all_comparisons$Probit[3], "V1", "V2"),
                                          ifelse(all_comparisons$Probit[2] > all_comparisons$Probit[4], "V1", "V2"),
                                          NA))

all_comparisons <- rbind(all_comparisons, new_row)


rownames(all_comparisons) <- c("Misclassifications Version 1", 
                               "Correct Winner Predictions Version 1",
                               "Misclassifications Version 2",
                               "Correct Winner Predictions Version 2",
                               "Version with Lowest Misclassifications",
                               "Version with Highest Correct Winner Predictions",
                               "Best Version")



# Reclassifying SVM Best Version so there's no ambiguity in the "Best Version" row.
all_comparisons$SVM.Radial[5] <- "V1"





for (i in 1:ncol(all_comparisons)) {
  
  if (all_comparisons[5, i] == all_comparisons[6, i]) {
    
    all_comparisons[7, i] <- all_comparisons[5, i]
    
  } else {
    
    all_comparisons[7, i] <- "Both"
    
  }
  
}



kable(all_comparisons, caption = "All Comparisons", align = rep("c", ncol(all_comparisons)))


```

As we can see from the "all_comparisons" data frame, the majority of models had one aspect better in a particular version and the other aspect better in the other version. This meant that it was hard to pick which model was best for each of the 8 model types. However, I felt that it was more important for a hybrid classification model to use the version that predicted the most winners, rather than the version that had the lowest number of misclassifications. 











# Hybriding Models

Another strategy for classification methods involves combining predictions of the various models and then classifying a particular shot to the majority class. For example, we have 8 models in use, so if at least 4 of the models predict a winner, then we would classify that shot as a winner. 

```{r}


version1 <- c("SVM.Linear", "Decision.Tree", "Logit", "Probit")
version2 <- c("Random.Forest", "SVM.Radial", "SVM.Polynomial", "XGBoost")


hybrid_comparison   <- cbind(comparison  %>% dplyr::select(Actual, SVM.Linear, Decision.Tree, Logit, Probit),
                             comparison2 %>% dplyr::select(Random.Forest, SVM.Radial, SVM.Polynomial, XGBoost))






for (j in 1:ncol(hybrid_comparison)) {

  hybrid_comparison[, j] <- as.numeric(as.character(hybrid_comparison[, j]))
  
}


hybrid_comparison$Majority <- NA


for (i in 1:nrow(hybrid_comparison)) {
  
  if (sum(hybrid_comparison[i, c(2:(ncol(hybrid_comparison)-1))]) >= 4) {
    
    hybrid_comparison$Majority[i] <- 1
    
  } else {
    
    hybrid_comparison$Majority[i] <- 0
    
  }
  
}



hybrid_comparison$Actual   <- as.factor(hybrid_comparison$Actual)
hybrid_comparison$Majority <- as.factor(hybrid_comparison$Majority)



(hybrid_table <- table(Actual    = hybrid_comparison$Actual,
                       Predicted = hybrid_comparison$Majority))



```

Unfortunately, the hybrid model based off a majority vote hasn't performed too well. It only correctly predicted `r hybrid_table[2,2]` winners and had a total of `r hybrid_table[1,2] + hybrid_table[2,1]` misclassifications. 


























# t-SNE Scatterplot and Parallel Coordinate Plot

A final method used to diagnose the issues with the classification models was through the use of t-SNE dimension reduction, which stands for t-Distriuted Stochastic Neighbour Embedding. This was conducted so that I could plot a 2D Scatterplot which uses a combination of many variables, which would show whether there are some problem cases that the data struggles with. This method was done using the "Rtsne" library. 

```{r tSNE}

X <- tr3 %>% select(-winner) %>% as.matrix

tsne <- Rtsne(X, dims = 2)

dim1 <- tsne$Y[,1]
dim2 <- tsne$Y[,2]


tr3 <- tr3 %>% mutate(MDS1 = dim1,
                      MDS2 = dim2)




myscale <- function(x) (x - mean(x)) / sd(x)

scale.dat.melt <- tr3 %>%
  mutate_each(funs(myscale), -winner) %>%
  mutate(ids = 1:nrow(tr3)) %>%
  gather(var, Value, -winner, -ids, convert=TRUE) %>%
  mutate(Variables = as.numeric(factor(var),
                                levels=c("speed1", "speed2",
                                         "ballmark.y", "end",
                                         "start.z", "final.y",
                                         "speed.diff", "oppo.hit.y",
                                         "oppo.hit.z", "oppo.speed", 
                                         "oppo.ballmark.y", "oppo_ballmark_x_ballmark_y",
                                         "speed.ratio", "short.dist", 
                                         "MDS1", "MDS2", "ids")))

colnames(scale.dat.melt)[1] <- "Class"


```





### Scatterplot

After creating the two reduced dimensions using t-SNE, it made sense to inspect a scatterplot of the dimensions and colour the points by the classification of winner and non-winner in order to identify any underlying issues with the classification task.  

```{r}

ggplot(data = tr3, aes(x = MDS1, y = MDS2, colour = winner)) +
      geom_point(size = I(3), alpha = 0.7)  +
      labs(y = "MDS 2", x = "MDS 1", title = "t-SNE Scatterplot") +
      theme(legend.key = element_blank(), aspect.ratio = 1, plot.title = element_text(hjust = 0.5))

```

As we can see, the points are all overlapping one another, which partially explains the issues that the models have had with classifying winners. 


### Parallel Coordinate Plot

A Parallel Coordinate Plot was also generated in order to identify any problem variables that weren't recognised earlier. The premise of the PCP is to see whether there is an overlap in the scaled values of the variables used or whether that particular variable actually shows some blatant separation between classes. 

```{r}

ggplot(scale.dat.melt, 
       aes(x = Variables, y = Value, group = ids, key = ids, colour = Class, var = var)) +
        geom_line(alpha = 0.5) +
        scale_x_discrete(limits = levels(as.factor(scale.dat.melt$var)), expand = c(0.01,0.01)) +
        ggtitle("Parallel Coordinate Plot") +
        theme(plot.title = element_text(hjust = 0.5), axis.text.x  = element_text(angle = 90, vjust = 0.5)) +
        scale_colour_brewer(type = "qual", palette = "Dark2")

```

Similarly to the scatterplot, we can see that the classes overlap one another for each class, which again provides an explanation of the performance of the models. 



# Interactive App

```{r}

# ui <- fluidPage(
# 
#    # Application title
#    titlePanel("Parallel coordinate plot linked to t-SNE plot"),
#    fluidRow(column(
#      width = 3,
#      align = "center",
#      plotlyOutput("mdsplot", width = 300, height = 300)), column(
#      width = 7, align = "center",
#      plotlyOutput("parallel", width=500, height = 400))
#    )
# )
# 
# 
# 
# 
# # Define server logic required to draw a histogram
# server <- function(input, output) {
# 
#   #Define reactive values for MDS plot and vote matrix plots
#   rv <- reactiveValues(data = data.frame(
#     MDS1 = tr3$MDS1, MDS2 = tr3$MDS2,
#     Class = tr3$winner, ids = 1:nrow(tr3),
#     fill = logical(nrow(tr3))))
# 
#   #function to update selected elements in each plot
#   updateRV <- function(selected) {
#     fill <- logical(length(rv$data$fill))
#     fill[selected] <- TRUE
#     rv$data$fill <- fill
#   }
# 
#   observeEvent(event_data("plotly_selected"),{
#     selected <- rv$data$ids %in% event_data("plotly_selected")$key
#     updateRV(selected)
#   })
# 
#   observeEvent(event_data("plotly_click"),{
#     k <- event_data("plotly_click")$key
#     if (any(k %in% unique(rv$data$ids))){
#       selected <- rv$data$ids %in% k
#     }
# 
#     updateRV(selected)
#   })
# 
#   output$parallel <- renderPlotly({
#     yy <- rv$data$ids[rv$data$fill]
# 
#       p <- ggplot(scale.dat.melt, aes(x = Variables, y = Value,
#                   group = ids, key = ids, colour = Class, var = var)) +
#         geom_line(alpha = 0.3) +
#         scale_x_discrete(limits = levels(as.factor(scale.dat.melt$var)),
#                          expand = c(0.01,0.01)) +
#         ggtitle("PCP") +
#         theme(legend.position = "none",
#               axis.text.x  = element_text(angle = 90, vjust = 0.5)) +
#         scale_colour_brewer(type = "qual", palette = "Dark2")
# 
#       if (length(yy) > 0) {
#         dat <-   scale.dat.melt %>% dplyr::filter(ids %in% yy)
#         p <- ggplot(scale.dat.melt, aes(x = Variables, y = Value,
#                 group = ids, key = ids, color = Class, var = var)) +
#           geom_line(alpha = 0.05) +
#           scale_x_discrete(limits = levels(as.factor(scale.dat.melt$var)),
#                            expand = c(0.01,0.01)) +
#           ggtitle("PCP") +
#             theme(legend.position = "none",
#                   axis.text.x  = element_text(angle = 90, vjust = 0.5)) +
#           scale_colour_brewer(type = "qual",palette = "Dark2")
# 
#         p <- p + geom_line(data = dat, size=1)
#       }
#     ggplotly(p,tooltip = c("var","colour","y","key")) %>%
#       layout(dragmode = "select")
#   })
# 
#   #MDS plot
#   output$mdsplot <- renderPlotly({
#     yy <- rv$data$ids[rv$data$fill]
# 
#     p <- ggplot(data = rv$data, aes(x = MDS1, y = MDS2,
#                                     colour = Class, key = ids)) +
#       geom_point(size = I(3), alpha = .5)  +
#       theme(legend.position = "none",
#             legend.text = element_text(angle = 90),
#             legend.key = element_blank(),
#             aspect.ratio = 1) +
#       labs(y = "MDS 2", x = "MDS 1",
#            title = "t-SNE") +
#       scale_colour_brewer(type = "qual",palette = "Dark2")
# 
#     if (length(yy) > 0) {
#       dat <- rv$data %>% dplyr::filter(ids %in% yy)
#       p <- ggplot(data = rv$data,
#                   aes(x = MDS1, y = MDS2, color = Class, key = ids)) +
#              geom_point(size = I(3), alpha = .1) +
#              theme(legend.position = "none",
#                    legend.text = element_text(angle = 90),
#                    legend.key = element_blank(), aspect.ratio = 1) +
#         labs(y = "MDS 2", x = "MDS 1", title = "t-SNE")  +
#         scale_colour_brewer(type =   "qual",palette = "Dark2")
# 
#       p <- p + geom_point(data = dat, size =  I(3))
# 
#     }
#     ggplotly(p,tooltip = c("colour","x","y","key")) %>% layout(dragmode = "select")
# 
#   })
# }
# 
# # Run the application
# shinyApp(ui = ui, server = server)


```








# Conclusion

Ultimately, the task at hand was difficult due to the lack of separation between the values of variables in the observations that were winners and those that weren't winners. This explained the generally poor performance of the classification methods, with each model having a test error rate of roughly 30-40%. The best model on the test set was probably the first version of the Linear SVM model, with a total number of `r svm_linear_table[2,2]` correctly predicted winners and only `r svm_linear_table[1,2] + svm_linear_table[2,1]` misclassifications. However, the model that correctly predicted the most winners was the second version of the Gradient Boosting model, with `r xgboost_table2[2,2]` correctly predicted winners out of the 111 winners in the test set. 

With more time available on this data set, I don't think I would have been able to get more encouraging results, as the t-SNE scatterplot and Parallel Coordinate Plot showed a lack of separation between classes for basically all variables. 

