---
title: "Winners and Errors Classification"
author: "Dean"
date: "2 Aug 2017"
output: html_document
---

```{r setup, include=FALSE}

knitr::opts_chunk$set(echo    = TRUE,
                      warning = FALSE, 
                      message = FALSE, 
                      error   = FALSE,
                      cache   = TRUE)

options(scipen = 999)

```



# Introduction

This project involved deep analysis into the aspects that distinguish winners from errors. The majority of shots in tennis end as errors, of which they are segmented between forced and unforced errors. However, using multiple machine learning algorithms and the ball-tracking data from Roger Federer's 2017 Australian Open matches, I attempted to see whether this task would achieve positive results, or whether the predictions were inaccurate. 






## Loading in Data Set

```{r load_data}

setwd("~/2017 Work/Federer/erroR/data")
load("~/2017 Work/Federer/erroR/data/federer2017.RData")

df <- federer2017



library(ggplot2)
library(tidyr)
library(gridExtra)
library(GGally)
library(e1071)
library(MASS)
library(dplyr)
library(randomForest)
library(gbm)
library(rpart)
library(rpart.plot)
library(penalizedLDA)
library(xgboost)
library(foreign)
library(stringr)
library(stringi)
library(devtools)
library(fpp2)
library(neuralnet)
library(caret)
library(PPtreeViz)
library(knitr)
library(ggthemes)
library(Rtsne)
library(shiny)
library(tourr)
library(tidyverse)
library(plotly)


```


## Data Manipulation

The data has some issues such as NAs, so these had to be filtered out before I ran any models. In addition, there are many variables that I created in order to enhance the variables going into the models. There are variables that relate to the angle of the shot, the opponent's shot that comes to the player and the attributes of that shot, such as placement and speed, etc.  

```{r adding_variables1}

### Removing NA Observations
df <- df %>% 
      filter(is.na(unforced) == FALSE) %>% 
      filter(is.na(ballmark.x) == FALSE) %>% 
      filter(is.na(net.clearance) == FALSE) %>%
      filter(is.na(hitpoint) == FALSE)


df$end_of_point          <- c(rep(NA, nrow(df)-1), 1)




# Working out which shot was the last shot for that rally 

for (i in 1:(nrow(df) - 1)) {
  
  if (df$id[i] != df$id[i+1]) {
    
    
    df$end_of_point[i] <- 1
    
    
  } else {
    
    
    df$end_of_point[i] <- 0
    
    
  }
  
  
  
}








df$ballmark_x_ballmark_y <- df$ballmark.x * df$ballmark.y       # Interaction variable
df$winner                <- as.factor(df$winner)
df$serve.classification  <- as.factor(df$serve.classification)
df$side                  <- as.factor(df$side)
df$hitpoint              <- as.factor(df$hitpoint)


df <- df %>% dplyr::select(-matchid, -serveid)


```





```{r adding_variables2}


#Adding variables 

df <- df %>%
  mutate(isserver = ifelse(server == impact.player,1,0),                 # Dummy for if the shot maker is the server
         retser   = ifelse(shot == 2, 1, 0),
         retser1  = ifelse(shot == 2 & serve.classification == 1, 1, 0),
         retser2  = ifelse(shot == 2 & serve.classification == 2, 1, 0))




for (i in 1:ncol(df)) {
  
  if(is.character(df[,i]) == TRUE) {
    
    df[,i] <- as.factor(df[,i])
    
  }
  
}




ids       <- df$id
split.ids <- t(data.frame(stri_split_boundaries(ids, type="character")))



df <- df %>%
      plyr::mutate(server.points   = as.numeric(split.ids[,1]), 
                   receiver.points = as.numeric(split.ids[,2]), 
                   server.games    = as.numeric(split.ids[,3]), 
                   receiver.games  = as.numeric(split.ids[,4]), 
                   server.sets     = as.numeric(split.ids[,5]), 
                   receiver.sets   = as.numeric(split.ids[,6]))




df <- df %>%
    mutate(server.sets.diff   = server.sets   - receiver.sets,
           server.games.diff  = server.games  - receiver.games,
           server.points.diff = server.points - receiver.points,
           speed.diff = NA, 
           oppo.hit.x = NA,
           oppo.hit.y = NA,
           oppo.hit.z = NA,
           oppo.speed = NA,
           oppo.ballmark.x = NA,
           oppo.ballmark.y = NA,
           oppo_ballmark_x_ballmark_y = NA,
           speed.ratio = NA)







# Adding variables for the shot coming towards the player hitting the winner

for (i in 2:nrow(df)) {
  
  if(df$shot[i] != 1) {             #so that only non-serves are affected

      df$speed.diff[i] = df$speed1[i] - df$speed1[i-1]  #speed difference
      
      df$oppo.hit.x[i]      <- df$start.x[i-1]
      df$oppo.hit.y[i]      <- df$start.y[i-1]
      df$oppo.hit.z[i]      <- df$start.z[i-1]
      df$oppo.speed[i]      <- df$speed1[i-1]
      df$oppo.ballmark.x[i] <- df$ballmark.x[i-1]
      df$oppo.ballmark.y[i] <- df$ballmark.y[i-1]
      
      df$oppo_ballmark_x_ballmark_y[i] <- df$oppo.ballmark.x[i] * df$oppo.ballmark.y[i]  #oppo hit
  
  }
  
}





# Speed ratio and distances to boundaries of the court

df <- df %>% 
  mutate(speed.ratio = speed1/oppo.speed) %>%             #speed ratio
  mutate(side.dist   = 4.115 - abs(oppo.ballmark.y)) %>%  #distance of oppo.ballmark from sideline
  mutate(base.dist   = 11.89 - abs(oppo.ballmark.x)) %>%  #distance of oppo.ballmark from baseline
  mutate(short.dist  = pmin(side.dist, base.dist)) %>%    #shortest distance from any line
  mutate(p.start.x   = NA) %>%
  mutate(p.start.y   = NA) %>%
  mutate(o.angle     = NA)




for (i in 3:nrow(df)) {
  
  if(df$impact.player[i] == df$impact.player[i-2] & df$shot[i] == df$shot[i-2] + 2) {
    
    df$p.start.x[i]=df$start.x[i-2]
    df$p.start.y[i]=df$start.y[i-2]
    
  }
  
}









# Adding the angle of the shots made
  
for (i in 1:nrow(df)) {
  
  x1 <- df$p.start.x[i]
  y1 <- df$p.start.y[i]
  x2 <- df$oppo.hit.x[i]
  y2 <- df$oppo.hit.y[i]
  x3 <- df$oppo.ballmark.x[i]
  y3 <- df$oppo.ballmark.y[i]
  
  o.angle <- acos(
  ((c(x1,y1)-c(x2,y2))/sqrt((x1-x2)^2+(y1-y2)^2)) %*%
    ((c(x2,y2)-c(x3,y3))/sqrt((x2-x3)^2+(y2-y3)^2))) * 180/pi
  
  df$o.angle[i] = ifelse(o.angle > 90, 180-o.angle, o.angle)
  
}







#Now adding the angle the player hits
df <- df %>%
  mutate(p.angle = NA)
  
for (i in 1:nrow(df)) {
  
  x1 <- df$oppo.hit.x[i]
  y1 <- df$oppo.hit.y[i]
  x2 <- df$start.x[i]
  y2 <- df$start.y[i]
  x3 <- df$ballmark.x[i]
  y3 <- df$ballmark.y[i]
  
  p.angle <- acos(
  ((c(x1,y1)-c(x2,y2))/sqrt((x1-x2)^2+(y1-y2)^2)) %*%
    ((c(x2,y2)-c(x3,y3))/sqrt((x2-x3)^2+(y2-y3)^2))) * 180/pi
  
  df$p.angle[i] = ifelse(p.angle > 90, 180-p.angle, p.angle)
  
}



```




# Illustration of a Rally

Before applying the modelling techniques, I thought it would be a good idea to demonstrate the progression of a particular rally. 

```{r fig.cap = "This plot shows the progression of a rally. The colours denote the two players and the thick line denotes the winner."}


# Creating a Data Frame of only 1 rally with exactly 6 shots
rally1 <- df %>% 
  filter(final.shot == 6 & winner == 1 & id == "125322") %>% 
  dplyr::select(impact.player, hitpoint, start.x, start.y, ballmark.x, ballmark.y, final.x, final.y)





# Removing the axes lines and labels
theme_court            <- theme_bw()
theme_court$line       <- element_blank()
theme_court$axis.text  <- element_blank()
theme_court$axis.title <- element_blank()




# Boundaries of the Court #
courtTrace <- data.frame(x = c(-11.89, -11.89, 0, 0, 0, 0, 0, 0, 11.89, 11.89, -11.89, -11.89, 11.89, 
                               11.89, -11.89, -6.4, -6.4, 6.4, 6.4, 6.4, -6.4),
                         y = c(5.49, -5.49, -5.49, -6.5, -6.5, 6.5, 6.5, -5.49, -5.49, 5.49, 5.49, 
                               4.115, 4.115, -4.115, -4.115, -4.115, 4.115, 4.115, -4.115, 0, 0),
                         z = c(0, 0, 0, 0, 1.09, 1.09, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0))


# Data frame of only the last shot of that rally (the winner)
final_shot <- rally1[-c(1:(nrow(rally1)-1)),]





# Illustration of Rally
ggplot() +  
  geom_path(data = courtTrace, 
            aes(x = x, y = y),
            color = 'black', size = 0.5) +
  
  geom_segment(data = courtTrace,
               aes(x = 0, xend = 0, y = -6.5, yend = 6.5),
               size = 0.7, 
               color = 'darkgrey', 
               lineend = 'round') +
  
  geom_path(aes(x = start.x, y = start.y, colour = impact.player, group = NA), 
            data = rally1, 
            arrow = arrow(length = unit(0.3,"cm"), ends = "last", type = "closed")) + 
  
  geom_segment(data = final_shot, 
               aes(x = start.x, y = start.y, xend = final.x, yend = final.y, colour = impact.player, group = NA), 
               size = 1.8, 
               arrow = arrow(length = unit(0.3, "cm"), ends = "last", type = "closed")) +
  theme_court + 
  guides(colour = guide_legend("Impact Player")) + 
  ggtitle(paste(nrow(rally1), " Point Rally Ending in a Winner", sep = "")) + 
  theme(plot.title = element_text(hjust = 0.5))


```

As we can see, in this particular rally, the rally started with a Federer serve and ended with a down-the-line winner by Rafael Nadal. The shot before the winner was a cross-court shot by Federer and you could see that he was on the complete opposite side of the court compared to the direction of the winner. In fact, the variables related to angle were added due to this notion. 





```{r}

# df_long <- df %>%
#             filter(end_of_point == 1) %>%
#             gather(key = vars, value = value, x01:z32)

```



```{r}

# ggplot(df_long) +
#   geom_boxplot(aes(x = as.factor(winner), y = value, fill = as.factor(winner))) +
#   facet_wrap(~vars, scale = "free_y")




```






## Filtering Data Set

In order to determine whether the last shot of the point was a winner or error, I filtered the data so that we are only looking at the last point of the rally. 

```{r}


# Filtering data frame to keep only observations that were the last shots of that rally and were not serves 
df <- df %>% filter(end_of_point == 1) %>% filter(hitpoint != "S")



# Removing unnecessary variables
df <- df %>% dplyr::select(-server.score,
                          -receiver.score,
                          -server.won,
                          -impact.player,
                          -server,
                          -receiver, 
                          -serve.classification,
                          -start2,
                          -end2, 
                          -forced, 
                          -unforced,
                          -end_of_point,
                          -p.start.x,
                          -p.start.y,
                          -o.angle,
                          -id)

```









# Classification Models
## Training and Test

The data was split into 2/3rds of training data and 1/3 of test data so that all the models could be tested properly. Using the *caret* package, there is a function called **createDataPartition** that samples the data frame and keeps the classes balanced in both the training set and test set. This means that 2/3rds of the total amount of winners appear in the training set and the remaining 1/3 of winners appear in the test set. 

Something that should be noted about the classification of winners is that all points are either winners or errors. What this means is that any prediction of a shot not being a winner means that the prediction is that the rally has ended in an error (either forced or unforced).

```{r training_and_test}

set.seed(10000)


# Creating the index for the data split
idx <- createDataPartition(df$winner, p = 2/3, list = FALSE, times = 1)

tr  <- df[idx,]
ts  <- df[-idx,]



ts2 <- federer2017[-idx,]





```












### Random Forest and Variable Selection Based Off Accuracy Measures

The Random Forest model was created first due to the additional features of the function that allow the user to determine the importance of the variables. This was ideal as there were a lot of variables used in the model and it was hard to determine whether a particular variable was significant or not.

```{r random_forest}

tr_rf   <- tr %>% 
            dplyr::select(-winner) %>% 
            randomForest(tr$winner,
                          ntree=1000, 
                          importance = TRUE)

plot(tr_rf, main = "Random Forest Model")
legend(650, 0.7, c("Winner Error Rate", "OOB Error Rate", "Non-Winner Error Rate"), fill = c(3,1,2))


```

We can see from the plot of the Random Forest error rate as a function of the number of trees that the Out-of-Bag error is roughly 40% for the majority of values, so in reality there is no need to fit 1000 trees. This value plateaus after roughly 200 trees, which is quite rapid. The next step is to predict the test set values using the generated Random Forest model.


```{r}

pred_rf <- predict(tr_rf, 
                   ts %>% dplyr::select(-winner), 
                   type="class") #Prediction using test data

df_rf <- data.frame(Actual = ts$winner, 
                    Predicted = pred_rf,
                    stringsAsFactors = FALSE)

(rf_table1 <- table(Actual = ts$winner, Predicted = pred_rf))




```

Based off the Out-of-Bag sample, the estimated error rate on the test set was deemed to be `r paste(round(tr_rf$err.rate[nrow(tr_rf$err.rate),1]*100, 2), "%", sep = "")`. The actual error rate on the test set ended up being `r paste((rf_table1[1,2] + rf_table1[2,1] / sum(rf_table1)) * 100, "%", sep = "")`. However, this Random Forest was conducted on the entire set of variables, so the next step would be to filter out potential noise variables and then refit the Random Forest model.

I created a Variable Importance Plot in order to identify any significant drops in variable importance, so that I could build a model solely using the signficiant variables. This is similar to a Scree Plot in Principal Component Analysis.

```{r}

rf_imp <- data.frame(Var = rownames(tr_rf$importance), tr_rf$importance, stringsAsFactors = FALSE) %>%
                     arrange(desc(MeanDecreaseAccuracy))

varImpPlot(tr_rf, sort = TRUE)

```

From here we can see that the variable "start.z" was generally the most important variable in the model, together with the "start.x" position. The variable "start.z" indicates the height of the ball when hit and "start.x" indicates the depth of the court when the shot is hit. This makes sense as the majority of smashes are at a increased height to an average shot, and a smash is more likely to be a winner than an ordinary shot. This is the same with the x position. If the player hitting the ball hits it from the baseline, he/she is less likely to hit a winner than if the ball has dropped short and closer to the net. From here, I filtered the variables based off a fairly arbitrary criterion. The variables that I kept were the ones that either had a "MeanDecreaseGini" of greater than 4.5 or a "MeanDecreaseAccuracy" of greater than 0.0003.

```{r filtering_columns_using_randomForest}


### Filtering the Columns Based Off Importance ###

rf_imp$dummy <- ifelse(rf_imp$MeanDecreaseGini > 4.5 | rf_imp$MeanDecreaseAccuracy > 0.0003, 1, 0)
rf_imp       <- rf_imp[which(rf_imp$dummy == 1),]
rf_var_list  <- rf_imp$Var
rf_var_list  <- append(rf_var_list, "winner")



### Keeping only the variables in "rf_var_list"

tr <- tr[, which(colnames(tr) %in% rf_var_list)]
ts <- ts[, which(colnames(ts) %in% rf_var_list)]

```









### Random Forest With Most Important Variables

Once the less important variables were filtered out, I refit the Random Forest model once again, hoping that the model would perform better. 

```{r random_forest2}

tr_rf   <- tr %>% 
            dplyr::select(-winner) %>% 
            randomForest(tr$winner,
                          ntree=1000, 
                          importance = TRUE)

plot(tr_rf, main = "Random Forest Model")
legend(650, 0.7, c("Winner Error Rate", "OOB Error Rate", "Non-Winner Error Rate"), fill = c(3,1,2))


```






```{r}

pred_rf <- predict(tr_rf, 
                   ts %>% dplyr::select(-winner), 
                   type="class") #Prediction using test data



df_rf <- data.frame(Actual = ts$winner, 
                    Predicted = pred_rf,
                    stringsAsFactors = FALSE)



(rf_table <- table(Actual = ts$winner, Predicted = pred_rf))

```

The classifier had `r rf_table[1,2] + rf_table[2,1]` misclassifications compared to `r rf_table1[1,2] + rf_table1[2,1]` previously. In this case, the classifier predicted an Out-of-Bag error rate of `r paste(round(tr_rf$err.rate[nrow(tr_rf$err.rate),1]*100, 2), "%", sep = "")`. The most positive result was seeing that the number of correctly predicted winners increased from `r rf_table1[2,2]` to `r rf_table[2,2]`. 





## SVM

The next idea was to use Support Vector Machines, which is a method that finds the boundary which gives the best separation between classes. 

### SVM Radial

The first method used was the "Radial" Support Vector Machine kernel. This method gives flexible boundaries, rather than just being a constant linear hyperplane. 

```{r svm_radial}

tr_svm_radial   <- svm(winner ~ ., 
                      data = tr, 
                      kernel = "radial")


pred_svm_radial <- predict(tr_svm_radial, ts)          
df_svm_radial   <- data.frame(Actual = ts$winner, 
                              Predicted = pred_svm_radial) 


(svm_radial_table <- table(ts$winner, pred_svm_radial))

```

As we can see, the Radial SVM had `r svm_radial_table[1,2] + svm_radial_table[2,1]` misclassifications, meaning an error rate of `r paste(round(((svm_radial_table[1,2] + svm_radial_table[2,1]) * 100) / sum(svm_radial_table), 2), "%", sep = "")`. The model correctly predicted `r svm_radial_table[2,2]` winners out of the 111 in the test set, which was fairly poor. 









### SVM Linear

The next Support Vector Machine kernel I used was the linear kernel. It provides the hyperplane with the best separation between classes. 

```{r svm_linear}

tr_svm_linear   <- svm(winner ~ ., 
                      data = tr, 
                      kernel = "linear")


pred_svm_linear <- predict(tr_svm_linear, ts)          
df_svm_linear   <- data.frame(Actual = ts$winner, 
                              Predicted = pred_svm_linear) 


(svm_linear_table <- table(ts$winner, pred_svm_linear))

```

This model correctly predicted `r svm_linear_table[2,2]` winner and had a total of `r svm_linear_table[1,2] + svm_linear_table[2,1]` misclassifications. We can see that the model has struggled to classify any winners. 










### SVM Polynomial

The final Support Vector Machine method used was a polynomial kernel with a degree of 2 (quadratic). 

```{r svm_polynomial}

tr_svm_polynomial   <- svm(winner ~ ., 
                      data = tr, 
                      kernel = "polynomial",
                      degree = 2)


pred_svm_polynomial <- predict(tr_svm_polynomial, ts)          
df_svm_polynomial   <- data.frame(Actual = ts$winner, 
                                  Predicted = pred_svm_polynomial) 


(svm_polynomial_table <- table(ts$winner, pred_svm_polynomial))

```

This method had `r svm_polynomial_table[1,2] + svm_polynomial_table[2,1]` misclassifications and `r svm_polynomial_table[2,2]` correctly predicted winners. Similarly to the previous SVM methods, the model has generally struggled to classify any winners. 


















### Decision Tree 

```{r decision_tree}

tr_decisiontree <- rpart(winner ~ ., 
                         data=tr,
                         method="class")         

prp(tr_decisiontree)     


```

The decision tree above is using many variables, with some of the variables being used multiple times. 



```{r}

pred_decisiontree <- predict(tr_decisiontree,              
                             ts, 
                             type="class")


df_decisiontree   <- data.frame(Actual    = ts$winner, 
                                Predicted = pred_decisiontree,
                                stringsAsFactors = FALSE) 


(decision_tree_table <- table(Actual    = ts$winner, 
                              Predicted = pred_decisiontree))

```

The decision tree had a total of `r decision_tree_table[1,2] + decision_tree_table[2,1]` misclassifications and correctly predicted `r decision_tree_table[2,2]` winners out of the 111. This was a fairly strong performance, but may have had such a high amount of errors due to overfitting issues.  












### XGBoost

Another model type that I used was a Gradient Boosting model, which is conducted through the R library *xgboost*. I set some of the parameters based off a trial and error methodology, so that's where the various values came from. 

```{r xgboost}


cls <- ifelse(tr[,"winner"] == 1, 2, 1)

dtrain <- tr %>% 
          dplyr::select(-winner) %>% 
          as.matrix %>% 
          xgb.DMatrix(label=cls)

param  <- list(max.depth = 8, eta = 0.5, silent = 1)
tr_xgb <- xgb.train(param, dtrain, nthread = 5, nround = 20)


ts_matrix <- ts %>% 
              dplyr::select(-winner) %>% 
              as.matrix

pxgb <- round(predict(tr_xgb, ts_matrix), 0)

pxgb <- ifelse(pxgb == 2, 1, 0)
pxgb <- as.factor(pxgb)




(xgboost_table <- table(Actual = ts$winner, Predicted = pxgb))

df_xgboost   <- data.frame(Actual    = ts$winner, 
                           Predicted = pxgb,
                           stringsAsFactors = FALSE) 



```

Overall, the Gradient Boosting algorithm performed very well. It correctly predicted `r xgboost_table[2,2]` winners but had a total of `r xgboost_table[1,2] + xgboost_table[2,1]` misclassifications, which was quite high. 







### Logit

The next model type is a Logistic Regression model, which is a form of regression for probabilities. The response variable is binary, with a 1 representing a winner and a 0 representing everything else. Due to the large number of variables in the training set, I also used a backwards stepwise regression, so that the model selected had a lower AIC value than the model with all variables. 

```{r logit}

tr_logit   <- glm(winner ~ ., family = binomial(link="logit"), data = tr)


tr_logit   <- suppressWarnings(step(tr_logit, direction = "backward", trace = FALSE))
tr_logit %>% summary


pred_logit <- predict(tr_logit, ts, type="response")
pred_logit <- ifelse(pred_logit >= 0.5, 1, 0)

df_logit   <- data.frame(Actual    = ts$winner, 
                         Predicted = pred_logit,
                         stringsAsFactors = FALSE) 


(logit_table <- table(Actual    = ts$winner, 
                      Predicted = pred_logit))



```

The Logistic Regression model has performed solidly, with an overall number of misclassifications of `r logit_table[1, 2] + logit_table[2, 1]` and with `r logit_table[2,2]` correctly predicted winners. 













### Probit

The probit model is another form of binary probability regression, with the response variable being a 1 if the shot was a winner and 0 for all other shots. 

```{r probit}
tr_probit   <- glm(winner ~ ., family = binomial(link="probit"), data = tr)


tr_probit   <- suppressWarnings(step(tr_probit, trace = FALSE))
tr_probit %>% summary


pred_probit <- predict(tr_probit, ts, type="response")
pred_probit <- ifelse(pred_probit >= 0.5, 1, 0)

df_probit   <- data.frame(Actual    = ts$winner, 
                          Predicted = pred_probit,
                          stringsAsFactors = FALSE) 


(probit_table  <- table(Actual    = ts$winner, 
                        Predicted = pred_probit))



```

The Probit model performed well, with the number of misclassifications being `r probit_table[1, 2] + probit_table[2, 1]` and the number of correctly predicted winners being `r probit_table[2, 2]`. 














### All Predictions Combined

In order to compare the results of each classification method, I've created a data frame of all predictions side by side called "comparison". In addition, I created the data frame "error_count" to show the total misclassifications for each class along with the number of correctly predicted aces, so the methods can be directly compared to one another. 

```{r comparison}
comparison <- data.frame(Actual            = ts$winner,
                         'Random Forest'   = df_rf$Predicted,
                         'SVM Radial'      = df_svm_radial$Predicted,
                         'SVM Linear'      = df_svm_linear$Predicted,
                         'SVM Polynomial'  = df_svm_polynomial$Predicted,
                         'Decision Tree'   = df_decisiontree$Predicted,
                         XGBoost           = df_xgboost$Predicted,
                         Logit             = df_logit$Predicted,
                         Probit            = df_probit$Predicted, stringsAsFactors = FALSE)




error_count <- data.frame('Random Forest'  = c(sum(comparison$Random.Forest  != comparison$Actual), rf_table[2, 2]),
                          'SVM Radial'     = c(sum(comparison$SVM.Radial     != comparison$Actual), svm_radial_table[2, 2]), 
                          'SVM Linear'     = c(sum(comparison$SVM.Linear     != comparison$Actual), svm_linear_table[2, 2]),
                          'SVM Polynomial' = c(sum(comparison$SVM.Polynomial != comparison$Actual), svm_polynomial_table[2, 2]),
                          'Decision Tree'  = c(sum(comparison$Decision.Tree  != comparison$Actual), decision_tree_table[2, 2]),
                          XGBoost          = c(sum(comparison$XGBoost        != comparison$Actual), xgboost_table[2, 2]),
                          Logit            = c(sum(comparison$Logit          != comparison$Actual), logit_table[2, 2]),
                          Probit           = c(sum(comparison$Probit         != comparison$Actual), probit_table[2, 2]), 
                          stringsAsFactors = FALSE)


rownames(error_count) <- c("Total Misclassifications", "Correct Winner Predictions")



error_count$Best <- c(ifelse(min(error_count[1,]) == error_count$Random.Forest[1],  "Random Forest",
                      ifelse(min(error_count[1,]) == error_count$SVM.Radial[1],     "SVM Radial",
                      ifelse(min(error_count[1,]) == error_count$SVM.Linear[1],     "SVM Linear",
                      ifelse(min(error_count[1,]) == error_count$SVM.Polynomial[1], "SVM Polynomial",
                      ifelse(min(error_count[1,]) == error_count$Decision.Tree[1],  "Decision Tree",
                      ifelse(min(error_count[1,]) == error_count$XGBoost[1],        "XGBoost", "Logit")))))),
                    
                      ifelse(max(error_count[2,]) == error_count$Random.Forest[2],  "Random Forest",
                      ifelse(max(error_count[2,]) == error_count$SVM.Radial[2],     "SVM Radial",
                      ifelse(max(error_count[2,]) == error_count$SVM.Linear[2],     "SVM Linear",
                      ifelse(max(error_count[2,]) == error_count$SVM.Polynomial[2], "SVM Polynomial",
                      ifelse(max(error_count[2,]) == error_count$Decision.Tree[2],  "Decision Tree",       
                      ifelse(max(error_count[2,]) == error_count$XGBoost[2],        "XGBoost", "Logit")))))))


kable(error_count, caption = "Model Performance", align = c("c","c","c","c","c","c","c","c","c","c"))
                         
```

As we can see, the model with the least amount of misclassifications was the `r error_count$Best[1]` model with `r min(error_count[1, 1:(ncol(error_count)-1)])` misclassifications out of the `r nrow(ts)` observations in the test set. This resulted in an error rate of `r paste(round((min(error_count[1, 1:(ncol(error_count)-1)]) * 100)/nrow(ts), 2), "%", sep = "")`.

After seeing these results, I decided to identify the reasons why the models had a fairly poor error rate. This is conducted in the next section. 






# Improvements to the Models

Part of the diagnosis of the model performance involved looking at a subset of observations that were either clearly predicted to be winners or clearly predicted to not be a winner. This was done in order to identify the variables that provided the best separation, so that a second version of classification algorithms could be conducted using a specific subset of the variables remaining in the training set. 

## Identifying Patterns in Clear Winners vs. Unpredictable Winners

I created a subset data frame of the test set that included observations that *were* winners and had at least 5 models predict a winner, or observations that *were not* winners and had no models predicting a winner. This data frame was called "identifying_trends". 

```{r}

identifying_trends <- cbind(ts,
                        comparison)
                        

identifying_trends$Pred_RF             <- ifelse(identifying_trends$Random.Forest == "0", 0, 1)
identifying_trends$Pred_SVM_Radial     <- ifelse(identifying_trends$SVM.Radial == "0", 0, 1)
identifying_trends$Pred_SVM_Linear     <- ifelse(identifying_trends$SVM.Linear == "0", 0, 1)
identifying_trends$Pred_SVM_Polynomial <- ifelse(identifying_trends$SVM.Polynomial == "0", 0, 1)
identifying_trends$Pred_XGBoost        <- ifelse(identifying_trends$XGBoost == "0", 0, 1)
identifying_trends$Pred_Decision_Tree  <- ifelse(identifying_trends$Decision.Tree == "0", 0, 1)


identifying_trends <- identifying_trends %>% 
                        dplyr::select(-Random.Forest, -SVM.Radial, -SVM.Linear, -SVM.Polynomial, -XGBoost, -Decision.Tree)

identifying_trends$countif   <- NA
identifying_trends$Dummy     <- NA




for (i in 1:nrow(identifying_trends)) {
  
  identifying_trends$countif[i] <- sum(identifying_trends[i, (ncol(identifying_trends)-8):(ncol(identifying_trends)-2)])
  
  if ((identifying_trends$countif[i] >= 5 && identifying_trends$Actual[i] == 1) || 
      (identifying_trends$countif[i] == 0 && identifying_trends$Actual[i] == 0)) {
    
    identifying_trends$Dummy[i] <- 1
    
  } else {
    
    identifying_trends$Dummy[i] <- 0
    
  }
  
}



identifying_trends <- identifying_trends %>% 
                        filter(Dummy == 1)


```

From here, I created various long format data frames from the "identifying_trends" data frame and inspected each variable one by one. 

```{r}

ident_trends_long1 <- identifying_trends %>% gather(key = variable, value = stat, seconds:z12)


ggplot(data = ident_trends_long1, aes(x = winner, y = stat)) + 
  geom_boxplot(aes(fill = winner)) + 
  facet_wrap(~variable, scales = "free")






ident_trends_long2 <- identifying_trends %>% gather(key = variable, value = stat, start:importance)


ggplot(data = ident_trends_long2, aes(x = winner, y = stat)) + 
  geom_boxplot(aes(fill = winner)) + 
  facet_wrap(~variable, scales = "free")







ident_trends_long3 <- identifying_trends %>% gather(key = variable, value = stat, ballmark_x_ballmark_y:p.angle)


ggplot(data = ident_trends_long3, aes(x = winner, y = stat)) + 
  geom_boxplot(aes(fill = winner)) + 
  facet_wrap(~variable, scales = "free")




```

Based off these various boxplots, I deemed that the following variables were the variables with the best separation:

* oppo.hit.y
* oppo.hit.z
* oppo.speed
* oppo.ballmark.y
* oppo_ballmark_x_ballmark_y
* short.dist
* speed.diff
* speed.ratio
* ballmark.y
* speed1
* speed2
* start.z
* end
* final.y

As a result, I decided to refit all classification models with this subset of variables.


# Final Classification Models

## Using the Stated Subset of Variables

```{r}

var_list <- c("oppo.hit.y", "oppo.hit.z", "oppo.speed", "oppo.ballmark.y", "oppo_ballmark_x_ballmark_y",
              "short.dist", "speed.diff", "speed.ratio", "ballmark.y", "speed1", "speed2", "start.z", 
              "end", "final.y", "winner")



tr3 <- tr[, which(colnames(tr) %in% var_list)]
ts3 <- ts[, which(colnames(ts) %in% var_list)]


```

After specifying the subset of variables to use, there were only `r ncol(tr3)-1` remaining for the models. 





### Random Forest With Most Important Variables

Once the less important variables were filtered out, I refit the Random Forest model once again. 

```{r random_forest3}

tr_rf2   <- tr3 %>% 
            dplyr::select(-winner) %>% 
            randomForest(tr3$winner,
                          ntree=1000, 
                          importance = TRUE)


plot(tr_rf2, main = "Random Forest Model")
legend(650, 0.7, c("Winner Error Rate", "OOB Error Rate", "Non-Winner Error Rate"), fill = c(3,1,2))


```

Similar to the previous version of the Random Forest model, the OOB Error Rate was roughly 40% and this plateaued after only 400 trees. 

```{r}

pred_rf2 <- predict(tr_rf2, 
                   ts3 %>% dplyr::select(-winner), 
                   type="class") #Prediction using test data



df_rf2 <- data.frame(Actual = ts3$winner, 
                    Predicted = pred_rf2,
                    stringsAsFactors = FALSE)


rf_imp2 <- data.frame(Var = rownames(tr_rf2$importance), tr_rf2$importance, stringsAsFactors = FALSE) %>%
          arrange(desc(MeanDecreaseAccuracy))


varImpPlot(tr_rf2, sort = TRUE)

```

With this subset of variables, the Variable Importance Plot still shows the variable "start.z" as being the most important, based off the "Mean Decrease Gini" index. The next most important variable was "ballmark.y", which relates to the y-position of the player's shot. 


```{r}

(rf_table2 <- table(Actual = ts3$winner, Predicted = pred_rf2))

```

In this case, the classifier predicted an Out-of-Bag error rate of `r paste(round(tr_rf2$err.rate[nrow(tr_rf2$err.rate),1]*100, 2), "%", sep = "")`, compared to `r paste(round(tr_rf$err.rate[nrow(tr_rf$err.rate),1]*100, 2), "%", sep = "")` in the first fit of the Random Forest. The number of misclassifications for this fit was `r rf_table2[1,2] + rf_table2[2,1]` compared to `r error_count$Random.Forest[1]` previously. In addition, the number of correctly predicted winners this time was `r rf_table2[2,2]` compared to `r error_count$Random.Forest[2]` previously. 




## SVM

### SVM Radial

```{r svm_radial2}

tr_svm_radial2   <- svm(winner ~ ., 
                      data = tr3, 
                      kernel = "radial")


pred_svm_radial2 <- predict(tr_svm_radial2, ts3)          
df_svm_radial2   <- data.frame(Actual = ts3$winner, 
                              Predicted = pred_svm_radial2) 


(svm_radial_table2 <- table(Actual = ts3$winner, Predicted = pred_svm_radial2))

```

The SVM Radial model had `r svm_radial_table2[2,1] + svm_radial_table2[1,2]` misclassifications in this fit compared to `r error_count$SVM.Radial[1]` previously. In addition, the new version correctly predicted `r svm_radial_table2[2, 2]` winners compared to `r svm_radial_table[2,2]` winners previously.








### SVM Linear

```{r svm_linear2}

tr_svm_linear2   <- svm(winner ~ ., 
                      data = tr3, 
                      kernel = "linear")


pred_svm_linear2 <- predict(tr_svm_linear2, ts3)          
df_svm_linear2   <- data.frame(Actual = ts3$winner, 
                              Predicted = pred_svm_linear2) 


(svm_linear_table2 <- table(Actual = ts3$winner, Predicted = pred_svm_linear2))

```

The SVM Linear model had only `r svm_linear_table2[1,2] + svm_linear_table2[2,1]` misclassifications, but didn't predict one winner, so it is a fairly useless model in this situation. The previous version of the model had `r error_count$SVM.Linear[1]` misclassifications but managed to correctly predict `r error_count$SVM.Linear[2]` winner.










### SVM Polynomial

```{r svm_polynomial2}

tr_svm_polynomial2   <- svm(winner ~ ., 
                       data = tr3, 
                       kernel = "polynomial",
                       degree = 2)


pred_svm_polynomial2 <- predict(tr_svm_polynomial2, ts3)          
df_svm_polynomial2   <- data.frame(Actual = ts3$winner, 
                                  Predicted = pred_svm_polynomial2) 


(svm_polynomial_table2 <- table(Actual = ts3$winner, Predicted = pred_svm_polynomial2))

```

The second version of the SVM Polynomial model performed worse than the first version. The total number of misclassifications was now `r svm_polynomial_table2[1,2] + svm_polynomial_table2[2,1]` compared to `r error_count$SVM.Polynomial[1]` misclassifications previously. In addition, the total number of correctly predicted winners was `r svm_polynomial_table2[2,2]` in the second version compared to `r error_count$SVM.Polynomial[2]` previously. 

















### Decision Tree 

```{r decision_tree2}

tr_decisiontree2 <- rpart(winner ~ ., 
                         data=tr3,
                         method="class")         

prp(tr_decisiontree2)     

```

In this version of the decision tree model, the model used a much smaller number of splits, which led to a high number of misclassifications previously, along with a high number of correct winner predictions. 

```{r}

pred_decisiontree2 <- predict(tr_decisiontree2,              
                             ts3, 
                             type="class")


df_decisiontree2   <- data.frame(Actual    = ts3$winner, 
                                Predicted = pred_decisiontree2,
                                stringsAsFactors = FALSE) 


(decision_tree_table2 <- table(Actual    = ts3$winner, 
                              Predicted = pred_decisiontree2))


```

The decision tree performed worse in the second version in terms of correct winner predictions, with a total of `r decision_tree_table2[2,2]` correctly predicted winners compared to `r error_count$Decision.Tree[2]` correctly predicted winners previously. In contrast, the total number of misclassifications this time around was `r decision_tree_table2[1,2] + decision_tree_table2[2,1]` compared to `r error_count$Decision.Tree[1]` previously. 













### XGBoost

The first version of the XGBoost model returned very promising results, so I was fairly optimistic about this second version performing well. 

```{r xgboost2}


cls <- ifelse(tr3[,"winner"] == 1, 2, 1)

dtrain <- tr3 %>% 
          dplyr::select(-winner) %>% 
          as.matrix %>% 
          xgb.DMatrix(label=cls)

param <- list(max.depth = 8, eta = 0.5, silent = 1)
tr_xgb2 <- xgb.train(param, dtrain, nthread = 5, nround = 20)


ts_matrix2 <- ts3 %>% 
              dplyr::select(-winner) %>% 
              as.matrix

pxgb2 <- round(predict(tr_xgb2, ts_matrix2), 0)

pxgb2 <- ifelse(pxgb2 == 2, 1, 0)
pxgb2 <- as.factor(pxgb2)




(xgboost_table2 <- table(Actual = ts3$winner, Predicted = pxgb2))


df_xgboost2   <- data.frame(Actual    = ts3$winner, 
                           Predicted = pxgb2,
                           stringsAsFactors = FALSE) 



```

The accuracy for the Gradient Boosting model decreased quite substantially, with `r xgboost_table2[2,2]` correctly predicted winners in the second version compared to `r error_count$XGBoost[2]` in the first version. The total number of misclassifications increased from `r error_count$XGBoost[1]` in version 1 to `r xgboost_table2[1,2] + xgboost_table2[2,1]` misclassifications in version 2. These results were fairly discouraging. 







### Logit


```{r logit2}

tr_logit2   <- glm(winner ~ ., family = binomial(link="logit"), data = tr3)


tr_logit2   <- suppressWarnings(step(tr_logit2, direction = "backward", trace = FALSE))
tr_logit2 %>% summary


pred_logit2 <- predict(tr_logit2, ts3, type="response")
pred_logit2 <- ifelse(pred_logit2 >= 0.5, 1, 0)

df_logit2   <- data.frame(Actual    = ts3$winner, 
                         Predicted = pred_logit2,
                         stringsAsFactors = FALSE) 


(logit_table2 <- table(Actual    = ts3$winner, 
                      Predicted = pred_logit2))



```

The Logit model performed worse in version 2, with `r logit_table2[2,2]` correctly predicted winners compared to `r error_count$Logit[2]` previously. In addition, the second version model had a total number of `r logit_table2[1,2] + logit_table2[2,1]` misclassifications compared to `r error_count$Logit[1]` misclassifications previously. 








### Probit

```{r probit2}
tr_probit2   <- glm(winner ~ ., family = binomial(link="probit"), data = tr3)
tr_probit2 %>% summary

tr_probit2   <- suppressWarnings(step(tr_probit2, trace = FALSE))



pred_probit2 <- predict(tr_probit2, ts3, type="response")
pred_probit2 <- ifelse(pred_probit2 >= 0.5, 1, 0)

df_probit2   <- data.frame(Actual    = ts3$winner, 
                          Predicted = pred_probit2,
                          stringsAsFactors = FALSE) 


(probit_table2  <- table(Actual    = ts3$winner, 
                        Predicted = pred_probit2))



```

The probit model also performed fairly poor, with only `r probit_table2[2,2]` correctly predicted winners, down from `r error_count$Probit[2]` in version 1. The misclassification rate also increased, with `r probit_table2[1,2] + probit_table2[2,1]` misclassifications compared to `r error_count$Probit[1]` previously. 





















### All Predictions Combined

Just as I did earlier in the report, I've created a data frame of all predictions side by side called "comparison2". In addition, I created the data frame "error_count2" to show the total misclassifications for each class along with the number of correctly predicted winners, so the methods can be directly compared to one another. 


```{r comparison2}

comparison2 <- data.frame(Actual            = ts3$winner,
                          'Random Forest'   = df_rf2$Predicted,
                          'SVM Radial'      = df_svm_radial2$Predicted,
                          'SVM Linear'      = df_svm_linear2$Predicted,
                          'SVM Polynomial'  = df_svm_polynomial2$Predicted,
                          'Decision Tree'   = df_decisiontree2$Predicted,
                          XGBoost           = df_xgboost2$Predicted,    
                          Logit             = df_logit2$Predicted,
                          Probit            = df_probit2$Predicted, stringsAsFactors = FALSE)




error_count2 <- data.frame('Random Forest'  = c(sum(comparison2$Random.Forest  != comparison2$Actual), rf_table2[2, 2]),
                           'SVM Radial'     = c(sum(comparison2$SVM.Radial     != comparison2$Actual), svm_radial_table2[2, 2]), 
                           'SVM Linear'     = c(sum(comparison2$SVM.Linear     != comparison2$Actual), svm_linear_table2[2, 2]),
                           'SVM Polynomial' = c(sum(comparison2$SVM.Polynomial != comparison2$Actual), svm_polynomial_table2[2, 2]),
                           'Decision Tree'  = c(sum(comparison2$Decision.Tree  != comparison2$Actual), decision_tree_table2[2, 2]),
                           XGBoost          = c(sum(comparison2$XGBoost        != comparison2$Actual), xgboost_table2[2, 2]),
                           Logit            = c(sum(comparison2$Logit          != comparison2$Actual), logit_table2[2, 2]),
                           Probit           = c(sum(comparison2$Probit         != comparison2$Actual), probit_table2[2, 2]), 
                           stringsAsFactors = FALSE)


rownames(error_count2) <- c("Total Misclassifications", "Correct Winner Predictions")



error_count2$Best <- c(ifelse(min(error_count2[1,]) == error_count2$Random.Forest[1],  "Random Forest",
                       ifelse(min(error_count2[1,]) == error_count2$SVM.Radial[1],     "SVM Radial",
                       ifelse(min(error_count2[1,]) == error_count2$SVM.Linear[1],     "SVM Linear",
                       ifelse(min(error_count2[1,]) == error_count2$SVM.Polynomial[1], "SVM Polynomial",
                       ifelse(min(error_count2[1,]) == error_count2$Decision.Tree[1],  "Decision Tree",       
                       ifelse(min(error_count2[1,]) == error_count2$XGBoost[1],        "XGBoost", "Logit")))))),
                    
                       ifelse(max(error_count2[2,]) == error_count2$Random.Forest[2],  "Random Forest",
                       ifelse(max(error_count2[2,]) == error_count2$SVM.Radial[2],     "SVM Radial",
                       ifelse(max(error_count2[2,]) == error_count2$SVM.Linear[2],     "SVM Linear",
                       ifelse(max(error_count2[2,]) == error_count2$SVM.Polynomial[2], "SVM Polynomial",
                       ifelse(max(error_count2[2,]) == error_count2$Decision.Tree[2],  "Decision Tree",
                       ifelse(max(error_count2[2,]) == error_count2$XGBoost[2],        "XGBoost", "Logit")))))))



kable(error_count2, caption = "Model Performance", align = c("c","c","c","c","c","c","c","c","c"))

                         
```

Ultimately, the best performing model in terms of the total number of misclassifications was the SVM Radial model, with `r error_count2$SVM.Radial[1]` misclassifications. However, closer look at the results of this method shows that it didn't actually predict any winners, so all the misclassifications are from the 111 winners in the test sample. 

The best method in terms of number of correctly predicted winners was the XGBoost model, with `error_count2$XGBoost[2]` correctly predicted winners (out of the 111 in the test sample). 


Ultimately, version two of the models were generally worse than the first version. 






## Comparison of Versions of Models

In order to determine which version of each model performed the best, it made sense to create a data frame that has the number of misclassifications for each version and the number of correctly predicted winners for each version and then compare the two scenarios. 

```{r}

all_comparisons <- rbind(error_count[,-ncol(error_count)], 
                         error_count2[,-ncol(error_count)])


new_row  <- data.frame(Random.Forest  = c(ifelse(all_comparisons$Random.Forest[1] < all_comparisons$Random.Forest[3], "V1", "V2"),
                                          ifelse(all_comparisons$Random.Forest[2] > all_comparisons$Random.Forest[4], "V1", "V2"),
                                          NA),
                       
                       SVM.Radial     = c(ifelse(all_comparisons$SVM.Radial[1] < all_comparisons$SVM.Radial[3], "V1", "V2"),
                                          ifelse(all_comparisons$SVM.Radial[2] > all_comparisons$SVM.Radial[4], "V1", "V2"),
                                          NA),
                       
                       SVM.Linear     = c(ifelse(all_comparisons$SVM.Linear[1] < all_comparisons$SVM.Linear[3], "V1", "V2"),
                                          ifelse(all_comparisons$SVM.Linear[2] > all_comparisons$SVM.Linear[4], "V1", "V2"),
                                          NA),
                       
                      SVM.Polynomial = c(ifelse(all_comparisons$SVM.Polynomial[1] < all_comparisons$SVM.Polynomial[3], "V1", "V2"),
                                         ifelse(all_comparisons$SVM.Polynomial[2] > all_comparisons$SVM.Polynomial[4], "V1", "V2"),
                                         NA),
                                          
                       Decision.Tree  = c(ifelse(all_comparisons$Decision.Tree[1] < all_comparisons$Decision.Tree[3], "V1", "V2"),
                                          ifelse(all_comparisons$Decision.Tree[2] > all_comparisons$Decision.Tree[4], "V1", "V2"),
                                          NA),
                       
                       XGBoost        = c(ifelse(all_comparisons$XGBoost[1] < all_comparisons$XGBoost[3], "V1", "V2"),
                                          ifelse(all_comparisons$XGBoost[2] > all_comparisons$XGBoost[4], "V1", "V2"),
                                          NA),
                                          
                       Logit          = c(ifelse(all_comparisons$Logit[1] < all_comparisons$Logit[3], "V1", "V2"),
                                          ifelse(all_comparisons$Logit[2] > all_comparisons$Logit[4], "V1", "V2"),
                                          NA),
                                          
                       Probit         = c(ifelse(all_comparisons$Probit[1] < all_comparisons$Probit[3], "V1", "V2"),
                                          ifelse(all_comparisons$Probit[2] > all_comparisons$Probit[4], "V1", "V2"),
                                          NA))

all_comparisons <- rbind(all_comparisons, new_row)


rownames(all_comparisons) <- c("Misclassifications Version 1", 
                               "Correct Winner Predictions Version 1",
                               "Misclassifications Version 2",
                               "Correct Winner Predictions Version 2",
                               "Version with Lowest Misclassifications",
                               "Version with Highest Correct Winner Predictions",
                               "Best Version")








for (i in 1:ncol(all_comparisons)) {
  
  if (all_comparisons[5, i] == all_comparisons[6, i]) {
    
    all_comparisons[7, i] <- all_comparisons[5, i]
    
  } else {
    
    all_comparisons[7, i] <- "Both"
    
  }
  
}



kable(all_comparisons, caption = "All Comparisons", align = rep("c", ncol(all_comparisons)))


```

As we can see from the "all_comparisons" data frame, the majority of models had one aspect better in a particular version and the other aspect better in the other version. This meant that it was hard to pick which model was best for each of the 8 model types. However, I felt that it was more important for a hybrid classification model to use the version that predicted the most winners, rather than the version that had the lowest number of misclassifications. 











# Hybriding Models

Another strategy for classification methods involves combining predictions of the various models and then classifying a particular shot to the majority class. For example, we have 8 models in use, so if at least 4 of the models predict a winner, then we would classify that shot as a winner. 

```{r}



hybrid_comparison   <- cbind(comparison  %>% dplyr::select(Actual, Random.Forest,      # Version 1 Models Selected
                                                           SVM.Linear, SVM.Polynomial, 
                                                           Decision.Tree, XGBoost, 
                                                           Logit, Probit),
                             
                             comparison2 %>% dplyr::select(SVM.Radial))                # Version 2 Models Selected






for (j in 1:ncol(hybrid_comparison)) {

  hybrid_comparison[, j] <- as.numeric(as.character(hybrid_comparison[, j]))
  
}


hybrid_comparison$Majority <- NA




# If 4 or more models had selected a Winner, then that particular observation was classified as a winner

for (i in 1:nrow(hybrid_comparison)) {
  
  if (sum(hybrid_comparison[i, c(2:(ncol(hybrid_comparison)-1))]) >= 4) {
    
    hybrid_comparison$Majority[i] <- 1
    
  } else {
    
    hybrid_comparison$Majority[i] <- 0
    
  }
  
}



hybrid_comparison$Actual   <- as.factor(hybrid_comparison$Actual)
hybrid_comparison$Majority <- as.factor(hybrid_comparison$Majority)



(hybrid_table <- table(Actual    = hybrid_comparison$Actual,
                       Predicted = hybrid_comparison$Majority))



```

Unfortunately, the hybrid model based off a majority vote hasn't performed too well. It only correctly predicted `r hybrid_table[2,2]` winners and had a total of `r hybrid_table[1,2] + hybrid_table[2,1]` misclassifications. 


























# t-SNE Scatterplot and Parallel Coordinate Plot

A final method used to diagnose the issues with the classification models was through the use of t-SNE dimension reduction, which stands for t-Distriuted Stochastic Neighbour Embedding. This was conducted so that I could plot a 2D Scatterplot which uses a combination of many variables, which would show whether there are some problem cases that the data struggles with. This method was done using the "Rtsne" library. 

```{r tSNE}

X <- tr3 %>% select(-winner) %>% as.matrix



#t-SNE dimension reduction (2 dimensions)
tsne <- Rtsne(X, dims = 2)

dim1 <- tsne$Y[,1]
dim2 <- tsne$Y[,2]


tr3 <- tr3 %>% mutate(MDS1 = dim1,
                      MDS2 = dim2)






# Creating a scaling function and data frame for the parallel coordinate plot
myscale <- function(x) (x - mean(x)) / sd(x)

scale.dat.melt <- tr3 %>%
  mutate_each(funs(myscale), -winner) %>%
  mutate(ids = 1:nrow(tr3)) %>%
  gather(var, Value, -winner, -ids, convert=TRUE) %>%
  mutate(Variables = as.numeric(factor(var),
                                levels=c("speed1", "speed2",
                                         "ballmark.y", "end",
                                         "start.z", "final.y",
                                         "speed.diff", "oppo.hit.y",
                                         "oppo.hit.z", "oppo.speed", 
                                         "oppo.ballmark.y", "oppo_ballmark_x_ballmark_y",
                                         "MDS1", "MDS2", "speed.ratio", "short.dist","ids")))

colnames(scale.dat.melt)[1] <- "Class"


```










### Scatterplots

After creating the two reduced dimensions using t-SNE, it made sense to inspect a scatterplot of the dimensions and colour the points by the classification of winner and non-winner in order to identify any underlying issues with the classification task.  

```{r}

p1 <- ggplot(data = tr3, aes(x = MDS1, y = MDS2, colour = winner)) +
  geom_point(size = I(3), alpha = 0.7)  +
  labs(y = "MDS 2", x = "MDS 1", title = "t-SNE Scatterplot") +
  theme(legend.key = element_blank(), aspect.ratio = 1, plot.title = element_text(hjust = 0.5))



ggplotly(p1)


```

As we can see, the points are all overlapping one another, which partially explains the issues that the models have had with classifying winners. 








### 3D MDS Scatterplot

While there was no separation between winners and non-winners in terms of the MDS values, it was also important to identify if there was any potential separation between the values in each class using three t-SNE dimensions in a 3D plot. 

```{r}

X <- tr3 %>% select(-winner, -MDS1, -MDS2) %>% as.matrix



#t-SNE dimension reduction (3 dimensions)
tsne <- Rtsne(X, dims = 3)

dim1 <- tsne$Y[,1]
dim2 <- tsne$Y[,2]
dim3 <- tsne$Y[,3]




mds_df <- tr3 %>% mutate(MDS1 = dim1,
                         MDS2 = dim2,
                         MDS3 = dim3)






(p <- plot_ly(data=mds_df, x = ~MDS1, y = ~MDS2, z = ~MDS3, color = ~winner, colors = c('#BF382A', '#0C4B8E'), type="scatter3d") %>% 
  add_markers() %>%
  layout(scene = list(xaxis = list(title = 'MDS1'),
                     yaxis = list(title = 'MDS2'),
                     zaxis = list(title = 'MDS3'))))





```

Unfortunately, the 3D plot showed that there was no separation between classes, which further explains the modelling performances earlier. 





### Parallel Coordinate Plot

A Parallel Coordinate Plot was also generated in order to identify any problem variables that weren't recognised earlier. The premise of the PCP is to see whether there is an overlap in the scaled values of the variables used or whether that particular variable actually shows some blatant separation between classes. 

```{r}

p1 <- ggplot(data = scale.dat.melt, 
       aes(x = Variables, y = Value, group = ids, key = ids, colour = Class, var = var)) +
  geom_line(alpha = 0.5) +
  scale_x_discrete(limits = levels(as.factor(scale.dat.melt$var)), expand = c(0.01,0.01)) +
  ggtitle("Parallel Coordinate Plot") +
  theme(plot.title = element_text(hjust = 0.5), axis.text.x  = element_text(angle = 90, vjust = 0.5)) +
  scale_colour_brewer(type = "qual", palette = "Dark2") 


ggplotly(p1)


```

Similarly to the scatterplot, we can see that the classes overlap one another for each class, which again provides an explanation of the performance of the models. 



# Interactive App

```{r}

# ui <- fluidPage(
# 
#    # Application title
#    titlePanel("Parallel coordinate plot linked to t-SNE plot"),
#    fluidRow(column(
#      width = 3,
#      align = "center",
#      plotlyOutput("mdsplot", width = 300, height = 300)), column(
#      width = 7, align = "center",
#      plotlyOutput("parallel", width=500, height = 400))
#    )
# )
# 
# 
# 
# 
# # Define server logic required to draw a histogram
# server <- function(input, output) {
# 
#   #Define reactive values for MDS plot and vote matrix plots
#   rv <- reactiveValues(data = data.frame(
#     MDS1 = tr3$MDS1, MDS2 = tr3$MDS2,
#     Class = tr3$winner, ids = 1:nrow(tr3),
#     fill = logical(nrow(tr3))))
# 
#   #function to update selected elements in each plot
#   updateRV <- function(selected) {
#     fill <- logical(length(rv$data$fill))
#     fill[selected] <- TRUE
#     rv$data$fill <- fill
#   }
# 
#   observeEvent(event_data("plotly_selected"),{
#     selected <- rv$data$ids %in% event_data("plotly_selected")$key
#     updateRV(selected)
#   })
# 
#   observeEvent(event_data("plotly_click"),{
#     k <- event_data("plotly_click")$key
#     if (any(k %in% unique(rv$data$ids))){
#       selected <- rv$data$ids %in% k
#     }
# 
#     updateRV(selected)
#   })
# 
#   output$parallel <- renderPlotly({
#     yy <- rv$data$ids[rv$data$fill]
# 
#       p <- ggplot(scale.dat.melt, aes(x = Variables, y = Value,
#                   group = ids, key = ids, colour = Class, var = var)) +
#         geom_line(alpha = 0.3) +
#         scale_x_discrete(limits = levels(as.factor(scale.dat.melt$var)),
#                          expand = c(0.01,0.01)) +
#         ggtitle("PCP") +
#         theme(legend.position = "none",
#               axis.text.x  = element_text(angle = 90, vjust = 0.5)) +
#         scale_colour_brewer(type = "qual", palette = "Dark2")
# 
#       if (length(yy) > 0) {
#         dat <-   scale.dat.melt %>% dplyr::filter(ids %in% yy)
#         p <- ggplot(scale.dat.melt, aes(x = Variables, y = Value,
#                 group = ids, key = ids, color = Class, var = var)) +
#           geom_line(alpha = 0.05) +
#           scale_x_discrete(limits = levels(as.factor(scale.dat.melt$var)),
#                            expand = c(0.01,0.01)) +
#           ggtitle("PCP") +
#             theme(legend.position = "none",
#                   axis.text.x  = element_text(angle = 90, vjust = 0.5)) +
#           scale_colour_brewer(type = "qual",palette = "Dark2")
# 
#         p <- p + geom_line(data = dat, size=1)
#       }
#     ggplotly(p,tooltip = c("var","colour","y","key")) %>%
#       layout(dragmode = "select")
#   })
# 
#   #MDS plot
#   output$mdsplot <- renderPlotly({
#     yy <- rv$data$ids[rv$data$fill]
# 
#     p <- ggplot(data = rv$data, aes(x = MDS1, y = MDS2,
#                                     colour = Class, key = ids)) +
#       geom_point(size = I(3), alpha = .5)  +
#       theme(legend.position = "none",
#             legend.text = element_text(angle = 90),
#             legend.key = element_blank(),
#             aspect.ratio = 1) +
#       labs(y = "MDS 2", x = "MDS 1",
#            title = "t-SNE") +
#       scale_colour_brewer(type = "qual",palette = "Dark2")
# 
#     if (length(yy) > 0) {
#       dat <- rv$data %>% dplyr::filter(ids %in% yy)
#       p <- ggplot(data = rv$data,
#                   aes(x = MDS1, y = MDS2, color = Class, key = ids)) +
#              geom_point(size = I(3), alpha = .1) +
#              theme(legend.position = "none",
#                    legend.text = element_text(angle = 90),
#                    legend.key = element_blank(), aspect.ratio = 1) +
#         labs(y = "MDS 2", x = "MDS 1", title = "t-SNE")  +
#         scale_colour_brewer(type =   "qual",palette = "Dark2")
# 
#       p <- p + geom_point(data = dat, size =  I(3))
# 
#     }
#     ggplotly(p,tooltip = c("colour","x","y","key")) %>% layout(dragmode = "select")
# 
#   })
# }
# 
# # Run the application
# shinyApp(ui = ui, server = server)


```








# Conclusion

Ultimately, the task at hand was difficult due to the lack of separation between the values of variables in the observations that were winners and those that weren't winners. This explained the generally poor performance of the classification methods, with each model having a test error rate of roughly 30-40%.  

With more time available on this data set, I don't think I would have been able to get more encouraging results, as the t-SNE scatterplots and Parallel Coordinate Plot showed a lack of separation between classes for basically all variables. 

There may have been other variables that could have been trialled, such as variables relating to "at-the-net" shots, which would have partially shown up in the variable "start.z" as these shots are usually smashes, but it makes sense that you expect there to be more winners if at least one of the players is standing close to the net. 

