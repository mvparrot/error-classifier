---
title: "Winners and Errors Classification"
author: "Dean"
date: "24 July 2017"
output: html_document
---

```{r setup, include=FALSE}

knitr::opts_chunk$set(echo    = TRUE,
                      warning = FALSE, 
                      message = FALSE, 
                      error   = FALSE,
                      cache   = FALSE)

options(scipen = 999)

```



## Loading in Data Set

```{r load_data}

setwd("~/2017 Work/Federer/erroR/data")
load("~/2017 Work/Federer/erroR/data/federer2017.RData")

df <- federer2017



library(ggplot2)
library(tidyr)
library(gridExtra)
library(GGally)
library(e1071)
library(MASS)
library(dplyr)
library(randomForest)
library(gbm)
library(rpart)
library(rpart.plot)
library(penalizedLDA)
library(xgboost)
library(foreign)
library(stringr)
library(stringi)
library(devtools)
library(fpp2)
library(neuralnet)
library(caret)
library(PPtreeViz)
library(knitr)
library(ggthemes)


```


## Data Manipulation

```{r adding_variables1}

### Removing NA Observations
df <- df %>% 
      filter(is.na(unforced) == FALSE) %>% 
      filter(is.na(ballmark.x) == FALSE) %>% 
      filter(is.na(net.clearance) == FALSE) %>%
      filter(is.na(hitpoint) == FALSE)






for (i in 1:nrow(df)) {
  
  if (df$serve.classification[i] == 0) {
    
    df$serve.classification[i] <- "Ace"
    
  } else if (df$serve.classification[i] == 1) {
    
    df$serve.classification[i] <- "ReturnInPlay"
    
  } else if (df$serve.classification[i] == 2) {
    
    df$serve.classification[i] <- "ReturnOutOfPlay"
    
  } else if (df$serve.classification[i] == 3) {
    
    df$serve.classification[i] <- "Fault"
    
  }
  
  
  
  
  
  
  
  
  
  if (federer2017$serve.classification[i] == 0) {
    
    federer2017$serve.classification[i] <- "Ace"
    
  } else if (federer2017$serve.classification[i] == 1) {
    
    federer2017$serve.classification[i] <- "ReturnInPlay"
    
  } else if (federer2017$serve.classification[i] == 2) {
    
    federer2017$serve.classification[i] <- "ReturnOutOfPlay"
    
  } else if (federer2017$serve.classification[i] == 3) {
    
    federer2017$serve.classification[i] <- "Fault"
    
  }
  

  
}







df$end_of_point          <- c(rep(NA, nrow(df)-1), 1)



for (i in 1:(nrow(df) - 1)) {
  
  if (df$id[i] != df$id[i+1]) {
    
    
    df$end_of_point[i] <- 1
    
    
  } else {
    
    
    df$end_of_point[i] <- 0
    
    
  }
  
  
  
}








df$ballmark_x_ballmark_y <- df$ballmark.x * df$ballmark.y
df$winner                <- as.factor(df$winner)
df$serve.classification  <- as.factor(df$serve.classification)
df$side                  <- as.factor(df$side)
df$hitpoint              <- as.factor(df$hitpoint)


df <- df %>% dplyr::select(-matchid, -serveid)


```





```{r adding_variables2}

#Reorienting shots so that all Federer's shot are from negative x side of court, and all opponents shots are from positive x side. y coordinates have been flipped as required.


#Federer
df <- df %>% rowwise %>%
  mutate(ballmark.x = ifelse(impact.player == "FEDERER" & start.x >= 0, -ballmark.x, ballmark.x)) %>%
  mutate(ballmark.y = ifelse(impact.player == "FEDERER" & start.x >= 0, -ballmark.y, ballmark.y)) %>%
  mutate(start.y    = ifelse(impact.player == "FEDERER" & start.x >= 0, -start.y, start.y)) %>%
  mutate(final.x    = ifelse(impact.player == "FEDERER" & start.x >= 0, -final.x, ballmark.x)) %>%
  mutate(final.y    = ifelse(impact.player == "FEDERER" & start.x >= 0, -final.y, ballmark.y)) %>%
  mutate(start.x    = ifelse(impact.player == "FEDERER" & start.x >= 0, -start.x, start.x))


#Opponents
df <- df %>% rowwise %>%
  mutate(start.y    = ifelse(impact.player != "FEDERER" & start.x <= 0, -start.y, start.y)) %>%
  mutate(ballmark.x = ifelse(impact.player != "FEDERER" & start.x <= 0, -ballmark.x, ballmark.x)) %>%
  mutate(ballmark.y = ifelse(impact.player != "FEDERER" & start.x <= 0, -ballmark.y, ballmark.y)) %>%
  mutate(final.x    = ifelse(impact.player != "FEDERER" & start.x <= 0, -final.x, ballmark.x)) %>%
  mutate(final.y    = ifelse(impact.player != "FEDERER" & start.x <= 0, -final.y, ballmark.y)) %>%
  mutate(start.x    = ifelse(impact.player != "FEDERER" & start.x <= 0, -start.x, start.x))


#Adding Vars

df <- df %>%
  mutate(isserver = ifelse(server == impact.player,1,0))
  
  
df <- df %>%
  mutate(retser  = ifelse(shot == 2, 1, 0)) %>%
  mutate(retser1 = ifelse(shot == 2 & serve.classification == 1, 1, 0)) %>%
  mutate(retser2 = ifelse(shot == 2 & serve.classification == 2, 1, 0))




for (i in 1:ncol(df)) {
  
  if(is.character(df[,i]) == TRUE) {
    
    df[,i] <- as.factor(df[,i])
    
  }
  
}




ids <- df$id
split.ids <- t(data.frame(stri_split_boundaries(ids, type="character")))



df <- df %>%
      plyr::mutate(server.points   = as.numeric(split.ids[,1]), 
                   receiver.points = as.numeric(split.ids[,2]), 
                   server.games    = as.numeric(split.ids[,3]), 
                   receiver.games  = as.numeric(split.ids[,4]), 
                   server.sets     = as.numeric(split.ids[,5]), 
                   receiver.sets   = as.numeric(split.ids[,6]))




#Now mutate this to Federer score and opponent score
df <- df %>% rowwise %>%
  mutate(fed.points = ifelse(server == "FEDERER", as.numeric(server.points),   as.numeric(receiver.points))) %>%
  mutate(fed.games  = ifelse(server == "FEDERER", as.numeric(server.games),    as.numeric(receiver.games))) %>%
  mutate(fed.sets   = ifelse(server == "FEDERER", as.numeric(server.sets),     as.numeric(receiver.sets))) %>%
  mutate(opp.points = ifelse(server == "FEDERER", as.numeric(receiver.points), as.numeric(server.points))) %>%
  mutate(opp.games  = ifelse(server == "FEDERER", as.numeric(receiver.games),  as.numeric(server.games))) %>%
  mutate(opp.sets   = ifelse(server == "FEDERER", as.numeric(receiver.sets),   as.numeric(server.sets)))



df <- df %>%
    mutate(sets.diff   = fed.sets - opp.sets) %>%
    mutate(games.diff  = fed.games - opp.games) %>%
    mutate(points.diff = fed.points - opp.points)



df <- df %>%
  mutate(speed.diff = NA) %>%
  mutate(oppo.hit.x = NA) %>%
  mutate(oppo.hit.y = NA) %>%
  mutate(oppo.hit.z = NA) %>%
  mutate(oppo.speed = NA) %>%
  mutate(oppo.ballmark.x = NA) %>%
  mutate(oppo.ballmark.y = NA) %>%
  mutate(oppo_ballmark_x_ballmark_y = NA) %>%
  mutate(speed.ratio = NA)





for (i in 2:nrow(df)) {
  
  if(df$shot[i] != 1) {             #so that only non-serves are affected

      df$speed.diff[i] = df$speed1[i] - df$speed1[i-1]
      #speed difference
      
      df$oppo.hit.x[i]      <- df$start.x[i-1]
      df$oppo.hit.y[i]      <- df$start.y[i-1]
      df$oppo.hit.z[i]      <- df$start.z[i-1]
      df$oppo.speed[i]      <- df$speed1[i-1]
      df$oppo.ballmark.x[i] <- df$ballmark.x[i-1]
      df$oppo.ballmark.y[i] <- df$ballmark.y[i-1]
      
      df$oppo_ballmark_x_ballmark_y[i] <- df$oppo.ballmark.x[i] * df$oppo.ballmark.y[i]
      #oppo hit
  
  }
  
}





df <- df %>%
  mutate(speed.ratio = speed1/oppo.speed) %>% #speed ratio
  mutate(side.dist   = 4.115 - abs(oppo.ballmark.y)) %>% #distance of oppo.ballmark from sideline
  mutate(base.dist   = 11.89 - abs(oppo.ballmark.x)) %>% #distance of oppo.ballmark from baseline
  mutate(short.dist  = min(side.dist, base.dist)) #shortest distance from any line




#Adding angles
df <- df %>%
  mutate(p.start.x = NA) %>%
  mutate(p.start.y = NA)

for (i in 3:nrow(df)) {
  
  if(df$impact.player[i] == df$impact.player[i-2] & df$shot[i] == df$shot[i-2] + 2) {
    
    df$p.start.x[i]=df$start.x[i-2]
    df$p.start.y[i]=df$start.y[i-2]
    
  }
  
}




#adding angle between fed.shot-opp.shot vector and opp.shot-opp.ballmark vector
#doing it in one line because df doesn't want to add vectors

df <- df %>%
  mutate(o.angle = NA)

  
for (i in 1:nrow(df)) {
  
  x1 <- df$p.start.x[i]
  y1 <- df$p.start.y[i]
  x2 <- df$oppo.hit.x[i]
  y2 <- df$oppo.hit.y[i]
  x3 <- df$oppo.ballmark.x[i]
  y3 <- df$oppo.ballmark.y[i]
  
  o.angle <- acos(
  ((c(x1,y1)-c(x2,y2))/sqrt((x1-x2)^2+(y1-y2)^2)) %*%
    ((c(x2,y2)-c(x3,y3))/sqrt((x2-x3)^2+(y2-y3)^2))) * 180/pi
  
  df$o.angle[i] = ifelse(o.angle > 90, 180-o.angle, o.angle)
  
}


#Now adding the angle the player hits
df <- df %>%
  mutate(p.angle = NA)
  
for (i in 1:nrow(df)) {
  
  x1 <- df$oppo.hit.x[i]
  y1 <- df$oppo.hit.y[i]
  x2 <- df$start.x[i]
  y2 <- df$start.y[i]
  x3 <- df$ballmark.x[i]
  y3 <- df$ballmark.y[i]
  
  p.angle <- acos(
  ((c(x1,y1)-c(x2,y2))/sqrt((x1-x2)^2+(y1-y2)^2)) %*%
    ((c(x2,y2)-c(x3,y3))/sqrt((x2-x3)^2+(y2-y3)^2))) * 180/pi
  
  df$p.angle[i] = ifelse(p.angle > 90, 180-p.angle, p.angle)
  
}



```








```{r}

# df_long <- df %>% 
#             filter(end_of_point == 1) %>%
#             gather(key = vars, value = value, x01:z32)

```



```{r}

# ggplot(df_long) + 
#   geom_boxplot(aes(x = as.factor(winner), y = value, fill = as.factor(winner))) + 
#   facet_wrap(~vars, scale = "free_y")




```






## Filtering Data Set

In order to determine whether the last shot of the point was a winner or error, I filtered the data so that we are only looking at the last point of the rally. 

```{r}

df <- df %>% filter(end_of_point == 1) %>% filter(hitpoint != "S")



df <- df %>% dplyr::select(-server.score,
                          -receiver.score,
                          -server.won,
                          -impact.player,
                          -server,
                          -receiver, 
                          -serve.classification,
                          -start2,
                          -end2, 
                          -forced, 
                          -unforced,
                          -end_of_point,
                          -p.start.x,
                          -p.start.y,
                          -o.angle,
                          -id)

```









# Classification Models
## Training and Test

```{r training_and_test}

set.seed(1000)
idx <- sample(nrow(df), nrow(df) * (2/3)) 

tr  <- df[idx,]
ts  <- df[-idx,]



ts2 <- federer2017[-idx,]





```












### Random Forest and Variable Selection Based Off Accuracy Measures

```{r random_forest}

tr_rf   <- tr %>% 
            dplyr::select(-winner) %>% 
            randomForest(tr$winner,
                          ntree=1000, 
                          importance = TRUE)

plot(tr_rf, main = "Random Forest Model")


pred_rf <- predict(tr_rf, 
                   ts %>% dplyr::select(-winner), 
                   type="class") #Prediction using test data

df_rf <- data.frame(Actual = ts$winner, 
                    Predicted = pred_rf,
                    stringsAsFactors = FALSE)

(rf_table1 <- table(Actual = ts$winner, Predicted = pred_rf))



rf_imp <- data.frame(Var = rownames(tr_rf$importance), tr_rf$importance, stringsAsFactors = FALSE) %>%
                     arrange(desc(MeanDecreaseAccuracy))

varImpPlot(tr_rf, sort = TRUE)





```

Based off the Out-of-Bag sample, the estimated error rate on the test set was deemed to be 34.67%. The actual error rate on the test set ended up being `r paste((rf_table1[1,2] + rf_table1[2,1] / sum(rf_table1)) * 100, "%", sep = "")`. However, this Random Forest was conducted on the entire set of variables, so the next step would be to filter out potential noise variables and then refit the Random Forest model. 

The variables that I kept were the ones that either had a "MeanDecreaseGini" of greater than 4.5 or a "MeanDecreaseAccuracy" of greater than 0.0003

```{r filtering_columns_using_randomForest}


### Filtering the Columns Based Off Importance ###

rf_imp$dummy <- ifelse(rf_imp$MeanDecreaseGini > 4.5 | rf_imp$MeanDecreaseAccuracy > 0.0003, 1, 0)
rf_imp       <- rf_imp[which(rf_imp$dummy == 1),]
rf_var_list  <- rf_imp$Var
rf_var_list  <- append(rf_var_list, "winner")



### Keeping only the variables in "rf_var_list"

tr <- tr[, which(colnames(tr) %in% rf_var_list)]
ts <- ts[, which(colnames(ts) %in% rf_var_list)]

```









### Random Forest With Most Important Variables

Once the less important variables were filtered out, I refit the Random Forest model once again. Frustratingly, this didn't really improve the results of the classifier. 

```{r random_forest2}

tr_rf   <- tr %>% 
            dplyr::select(-winner) %>% 
            randomForest(tr$winner,
                          ntree=1000, 
                          importance = TRUE)

plot(tr_rf, main = "Random Forest Model")


pred_rf <- predict(tr_rf, 
                   ts %>% dplyr::select(-winner), 
                   type="class") #Prediction using test data



df_rf <- data.frame(Actual = ts$winner, 
                    Predicted = pred_rf,
                    stringsAsFactors = FALSE)



(rf_table <- table(Actual = ts$winner, Predicted = pred_rf))



rf_imp <- data.frame(Var = rownames(tr_rf$importance), tr_rf$importance, stringsAsFactors = FALSE) %>%
          arrange(desc(MeanDecreaseAccuracy))


varImpPlot(tr_rf, sort = TRUE)

```

In this case, the classifier predicted an Out-of-Bag error rate of 34.23%, compared to 34.67% in the first fit of the Random Forest. In addition, the test set error was the exact same as earlier, with an error rate of `r paste((rf_table[1,2] + rf_table[2,1] / sum(rf_table)) * 100, "%", sep = "")`. In fact, this refined Random Forest model had the exact same predictions as the last model. 





## SVM

### SVM Radial

```{r svm_radial}

tr_svm_radial   <- svm(winner ~ ., 
                      data = tr, 
                      kernel = "radial")


pred_svm_radial <- predict(tr_svm_radial, ts)          
df_svm_radial   <- data.frame(Actual = ts$winner, 
                              Predicted = pred_svm_radial) 


(svm_radial_table <- table(ts$winner, pred_svm_radial))

```











### SVM Linear

```{r svm_linear}

tr_svm_linear   <- svm(winner ~ ., 
                      data = tr, 
                      kernel = "linear")


pred_svm_linear <- predict(tr_svm_linear, ts)          
df_svm_linear   <- data.frame(Actual = ts$winner, 
                              Predicted = pred_svm_linear) 


(svm_linear_table <- table(ts$winner, pred_svm_linear))

```












### SVM Polynomial

```{r svm_polynomial}

tr_svm_polynomial   <- svm(winner ~ ., 
                      data = tr, 
                      kernel = "polynomial",
                      degree = 2)


pred_svm_polynomial <- predict(tr_svm_polynomial, ts)          
df_svm_polynomial   <- data.frame(Actual = ts$winner, 
                                  Predicted = pred_svm_polynomial) 


(svm_polynomial_table <- table(ts$winner, pred_svm_polynomial))

```




















### Decision Tree 

```{r decision_tree}

tr_decisiontree <- rpart(winner ~ ., 
                         data=tr,
                         method="class")         

prp(tr_decisiontree)     



pred_decisiontree <- predict(tr_decisiontree,              
                             ts, 
                             type="class")


df_decisiontree   <- data.frame(Actual    = ts$winner, 
                                Predicted = pred_decisiontree,
                                stringsAsFactors = FALSE) 


(decision_tree_table <- table(Actual    = ts$winner, 
                              Predicted = pred_decisiontree))


```
















### XGBoost

```{r xgboost}


cls <- ifelse(tr[,"winner"] == 1, 2, 1)

dtrain <- tr %>% 
          dplyr::select(-winner) %>% 
          as.matrix %>% 
          xgb.DMatrix(label=cls)

param <- list(max.depth = 2, eta = 1, silent = 1)
tr_xgb <- xgb.train(param, dtrain, nthread = 2, nround = 10)


ts_matrix <- ts %>% 
              dplyr::select(-winner) %>% 
              as.matrix

pxgb <- round(predict(tr_xgb, ts_matrix), 0)

pxgb <- ifelse(pxgb == 2, 1, 0)
pxgb <- as.factor(pxgb)




(xgboost_table <- table(Actual = ts$winner, Predicted = pxgb))

df_xgboost   <- data.frame(Actual    = ts$winner, 
                           Predicted = pxgb,
                           stringsAsFactors = FALSE) 



```









### Logit

It was important to explore the Logistic Regression and Probit models due to the separation of some variables in the data with respect to the Serve Classification. For example, if we look at a plot of the speed of a serve and distinguish the serves by "Ace" or "Other", we can see a distinct relationship between the two. This plot is shown below.

```{r binary_plot_speed}

# ggplot(data = tr, aes(x = speed, y = ifelse(serve_classification == "Ace", 1, 0), colour = serve_classification)) + 
#   geom_point() +
#   xlab("Speed") + 
#   ylab("Ace/Other") + 
#   ggtitle("Aces by Speed") + 
#   theme(plot.title = element_text(hjust = 0.5))

```



```{r logit}

tr_logit   <- glm(winner ~ ., family = binomial(link="logit"), data = tr)


tr_logit   <- suppressWarnings(step(tr_logit, direction = "backward", trace = FALSE))
tr_logit %>% summary


pred_logit <- predict(tr_logit, ts, type="response")
pred_logit <- ifelse(pred_logit >= 0.5, 1, 0)

df_logit   <- data.frame(Actual    = ts$winner, 
                         Predicted = pred_logit,
                         stringsAsFactors = FALSE) 


(logit_table <- table(Actual    = ts$winner, 
                      Predicted = pred_logit))



```

The Logistic Regression model has performed extremely well, with an overall misclassification error of `r logit_table[1, 2] + logit_table[2, 1]`. Encouragingly, the actual number of correctly predicted aces was also impressive, with the model correctly predicting `r logit_table[2, 2]` out of the `r sum(logit_table[1, ])` aces in the training set. 













### Probit

```{r probit}
tr_probit   <- glm(winner ~ ., family = binomial(link="probit"), data = tr)
tr_probit %>% summary

tr_probit   <- suppressWarnings(step(tr_probit, trace = FALSE))



pred_probit <- predict(tr_probit, ts, type="response")
pred_probit <- ifelse(pred_probit >= 0.5, 1, 0)

df_probit   <- data.frame(Actual    = ts$winner, 
                          Predicted = pred_probit,
                          stringsAsFactors = FALSE) 


(probit_table  <- table(Actual    = ts$winner, 
                        Predicted = pred_probit))



```

The Probit model didn't perform as strongly as the Logistic Regression model, with the number of misclassifications being `r probit_table[1, 2] + probit_table[2, 1]` and the number of correctly predicted aces being `r probit_table[1, 1]`. 














### All Predictions Combined

In order to compare the results of each classification method, I've created a data frame of all predictions side by side called "comparison". In addition, I created the data frame "error_count" to show the total misclassifications for each class along with the number of correctly predicted aces, so the methods can be directly compared to one another. 

```{r comparison}
comparison <- data.frame(Actual            = ts$winner,
                         'Random Forest'   = df_rf$Predicted,
                         'SVM Radial'      = df_svm_radial$Predicted,
                         'SVM Linear'      = df_svm_linear$Predicted,
                         'SVM Polynomial'  = df_svm_polynomial$Predicted,
                         'Decision Tree'   = df_decisiontree$Predicted,
                         XGBoost           = df_xgboost$Predicted,
                         Logit             = df_logit$Predicted,
                         Probit            = df_probit$Predicted, stringsAsFactors = FALSE)




error_count <- data.frame('Random Forest'  = c(sum(comparison$Random.Forest  != comparison$Actual), rf_table[2, 2]),
                          'SVM Radial'     = c(sum(comparison$SVM.Radial     != comparison$Actual), svm_radial_table[2, 2]), 
                          'SVM Linear'     = c(sum(comparison$SVM.Linear     != comparison$Actual), svm_linear_table[2, 2]),
                          'SVM Polynomial' = c(sum(comparison$SVM.Polynomial != comparison$Actual), svm_polynomial_table[2, 2]),
                          'Decision Tree'  = c(sum(comparison$Decision.Tree  != comparison$Actual), decision_tree_table[2, 2]),
                          XGBoost          = c(sum(comparison$XGBoost        != comparison$Actual), xgboost_table[2, 2]),
                          Logit            = c(sum(comparison$Logit          != comparison$Actual), logit_table[2, 2]),
                          Probit           = c(sum(comparison$Probit         != comparison$Actual), probit_table[2, 2]), 
                          stringsAsFactors = FALSE)


rownames(error_count) <- c("Total Misclassifications", "Correct Winner Predictions")



error_count$Best <- c(ifelse(min(error_count[1,]) == error_count$Random.Forest[1],  "Random Forest",
                      ifelse(min(error_count[1,]) == error_count$SVM.Radial[1],     "SVM Radial",
                      ifelse(min(error_count[1,]) == error_count$SVM.Linear[1],     "SVM Linear",
                      ifelse(min(error_count[1,]) == error_count$SVM.Polynomial[1], "SVM Polynomial",
                      ifelse(min(error_count[1,]) == error_count$Decision.Tree[1],  "Decision Tree",
                      ifelse(min(error_count[1,]) == error_count$XGBoost[1],        "XGBoost", "Logit")))))),
                    
                      ifelse(max(error_count[2,]) == error_count$Random.Forest[2],  "Random Forest",
                      ifelse(max(error_count[2,]) == error_count$SVM.Radial[2],     "SVM Radial",
                      ifelse(max(error_count[2,]) == error_count$SVM.Linear[2],     "SVM Linear",
                      ifelse(max(error_count[2,]) == error_count$SVM.Polynomial[2], "SVM Polynomial",
                      ifelse(max(error_count[2,]) == error_count$Decision.Tree[2],  "Decision Tree",       
                      ifelse(max(error_count[2,]) == error_count$XGBoost[2],        "XGBoost", "Logit")))))))


kable(error_count, caption = "Model Performance", align = c("c","c","c","c","c","c","c","c","c","c"))
                         
```









## Identifying Patterns in Clear Winners vs. Unpredictable Winners

```{r}

identifying_trends <- cbind(ts,
                        comparison)
                        

identifying_trends$Pred_RF             <- ifelse(identifying_trends$Random.Forest == "0", 0, 1)
identifying_trends$Pred_SVM_Radial     <- ifelse(identifying_trends$SVM.Radial == "0", 0, 1)
identifying_trends$Pred_SVM_Linear     <- ifelse(identifying_trends$SVM.Linear == "0", 0, 1)
identifying_trends$Pred_SVM_Polynomial <- ifelse(identifying_trends$SVM.Polynomial == "0", 0, 1)
identifying_trends$Pred_XGBoost        <- ifelse(identifying_trends$XGBoost == "0", 0, 1)


identifying_trends <- identifying_trends %>% 
                        dplyr::select(-Random.Forest, -SVM.Radial, -SVM.Linear, -SVM.Polynomial, -XGBoost)

identifying_trends$countif   <- NA
identifying_trends$Dummy     <- NA




for (i in 1:nrow(identifying_trends)) {
  
  identifying_trends$countif[i] <- sum(identifying_trends[i, (ncol(identifying_trends)-8):(ncol(identifying_trends)-2)])
  
  if ((identifying_trends$countif[i] >= 5 && identifying_trends$Actual[i] == 1) || 
      (identifying_trends$countif[i] == 0 && identifying_trends$Actual[i] == 0)) {
    
    identifying_trends$Dummy[i] <- 1
    
  } else {
    
    identifying_trends$Dummy[i] <- 0
    
  }
  
}



identifying_trends <- identifying_trends %>% 
                        filter(Dummy == 1)


```




```{r}

ident_trends_long1 <- identifying_trends %>% gather(key = variable, value = stat, seconds:z32)


ggplot(data = ident_trends_long1, aes(x = winner, y = stat)) + 
  geom_boxplot(aes(fill = winner)) + 
  facet_wrap(~variable, scales = "free")






ident_trends_long2 <- identifying_trends %>% gather(key = variable, value = stat, start:importance)


ggplot(data = ident_trends_long2, aes(x = winner, y = stat)) + 
  geom_boxplot(aes(fill = winner)) + 
  facet_wrap(~variable, scales = "free")







ident_trends_long3 <- identifying_trends %>% gather(key = variable, value = stat, ballmark_x_ballmark_y:p.angle)


ggplot(data = ident_trends_long3, aes(x = winner, y = stat)) + 
  geom_boxplot(aes(fill = winner)) + 
  facet_wrap(~variable, scales = "free")




```

Based off these various boxplots, I deemed that the following variables were the variables with the best separation:

* oppo.hit.y
* oppo.hit.z
* oppo.speed
* oppo.ballmark.y
* oppo_ballmark_x_ballmark_y
* short.dist
* speed.diff
* speed.ratio
* ballmark.y
* speed1
* speed2
* start.z
* end
* final.y

As a result, I decided to refit all classification models with this subset of variables.


# Final Classification Models

## Using the Stated Subset of Variables

```{r}

var_list <- c("oppo.hit.y", "oppo.hit.z", "oppo.speed", "oppo.ballmark.y", "oppo_ballmark_x_ballmark_y",
              "short.dist", "speed.diff", "speed.ratio", "ballmark.y", "speed1", "speed2", "start.z", 
              "end", "final.y", "winner")



tr3 <- tr[, which(colnames(tr) %in% var_list)]
ts3 <- ts[, which(colnames(ts) %in% var_list)]


```







### Random Forest With Most Important Variables

Once the less important variables were filtered out, I refit the Random Forest model once again. Frustratingly, this didn't really improve the results of the classifier. 

```{r random_forest3}

tr_rf2   <- tr3 %>% 
            dplyr::select(-winner) %>% 
            randomForest(tr3$winner,
                          ntree=1000, 
                          importance = TRUE)

plot(tr_rf2, main = "Random Forest Model")


pred_rf2 <- predict(tr_rf2, 
                   ts3 %>% dplyr::select(-winner), 
                   type="class") #Prediction using test data



df_rf2 <- data.frame(Actual = ts3$winner, 
                    Predicted = pred_rf2,
                    stringsAsFactors = FALSE)



(rf_table2 <- table(Actual = ts3$winner, Predicted = pred_rf2))



rf_imp2 <- data.frame(Var = rownames(tr_rf2$importance), tr_rf2$importance, stringsAsFactors = FALSE) %>%
          arrange(desc(MeanDecreaseAccuracy))


varImpPlot(tr_rf2, sort = TRUE)

```

In this case, the classifier predicted an Out-of-Bag error rate of 33.48%, compared to 34.67% in the first fit of the Random Forest. This was quite a substantial improvement. The test set error was the exact same as earlier, with an error rate of `r paste((rf_table2[1,2] + rf_table2[2,1] / sum(rf_table2)) * 100, "%", sep = "")`. This was slightly better than the previous Random Forest model.





## SVM

### SVM Radial

```{r svm_radial2}

tr_svm_radial2   <- svm(winner ~ ., 
                      data = tr3, 
                      kernel = "radial")


pred_svm_radial2 <- predict(tr_svm_radial2, ts3)          
df_svm_radial2   <- data.frame(Actual = ts3$winner, 
                              Predicted = pred_svm_radial2) 


(svm_radial_table2 <- table(Actual = ts3$winner, Predicted = pred_svm_radial2))

```

The SVM Radial model had the same number of misclassifications as before, but this time the number of correctly predicted winners actually decreased. Therefore, the model was worse than previously. 









### SVM Linear

```{r svm_linear2}

tr_svm_linear2   <- svm(winner ~ ., 
                      data = tr3, 
                      kernel = "linear")


pred_svm_linear2 <- predict(tr_svm_linear2, ts3)          
df_svm_linear2   <- data.frame(Actual = ts3$winner, 
                              Predicted = pred_svm_linear2) 


(svm_linear_table2 <- table(Actual = ts3$winner, Predicted = pred_svm_linear2))

```

The SVM Linear model had only `r svm_linear_table2[1,2] + svm_linear_table2[2,1]` misclassifications, but didn't predict one winner, so it is a fairly useless model in this situation. This was a significant decrease in performance compared to the earlier fit, which had only `r error_count$SVM.Linear[1]` misclassifications and correctly predicted `r error_count$SVM.Linear[2]` winners. 










### SVM Polynomial

```{r svm_polynomial2}

tr_svm_polynomial2   <- svm(winner ~ ., 
                       data = tr3, 
                       kernel = "polynomial",
                       degree = 2)


pred_svm_polynomial2 <- predict(tr_svm_polynomial2, ts3)          
df_svm_polynomial2   <- data.frame(Actual = ts3$winner, 
                                  Predicted = pred_svm_polynomial2) 


(svm_polynomial_table2 <- table(Actual = ts3$winner, Predicted = pred_svm_polynomial2))

```

Similarly to the SVM Linear model, the SVM Polynomial model has performed worse than before. The total number of misclassifications is `r svm_polynomial_table2[1, 2] + svm_polynomial_table2[2, 1]` compared to `r error_count$SVM.Polynomial[1]` previously. In addition, the total number of correctly predicted winners was `r svm_polynomial_table2[2, 2]` compared to `r error_count$SVM.Polynomial[2]` previously. 



















### Decision Tree 

```{r decision_tree2}

tr_decisiontree2 <- rpart(winner ~ ., 
                         data=tr3,
                         method="class")         

prp(tr_decisiontree2)     



pred_decisiontree2 <- predict(tr_decisiontree2,              
                             ts3, 
                             type="class")


df_decisiontree2   <- data.frame(Actual    = ts3$winner, 
                                Predicted = pred_decisiontree2,
                                stringsAsFactors = FALSE) 


(decision_tree_table2 <- table(Actual    = ts3$winner, 
                              Predicted = pred_decisiontree2))


```

Previously, the Decision Tree model performed so poorly that I didn't include the results in the comparison data frame. However, this instance, the total number of misclassifications has reduced to `r decision_tree_table[1, 2] + decision_tree_table[2, 1]` and encouragingly, the total number of correctly predicted winners has reached `r decision_tree_table[2, 2]`, which is higher than any previous model. 














### XGBoost

```{r xgboost2}


cls <- ifelse(tr3[,"winner"] == 1, 2, 1)

dtrain <- tr3 %>% 
          dplyr::select(-winner) %>% 
          as.matrix %>% 
          xgb.DMatrix(label=cls)

param <- list(max.depth = 2, eta = 1, silent = 1)
tr_xgb2 <- xgb.train(param, dtrain, nthread = 2, nround = 10)


ts_matrix2 <- ts3 %>% 
              dplyr::select(-winner) %>% 
              as.matrix

pxgb2 <- round(predict(tr_xgb2, ts_matrix2), 0)

pxgb2 <- ifelse(pxgb2 == 2, 1, 0)
pxgb2 <- as.factor(pxgb2)




(xgboost_table2 <- table(Actual = ts3$winner, Predicted = pxgb2))


df_xgboost2   <- data.frame(Actual    = ts3$winner, 
                           Predicted = pxgb2,
                           stringsAsFactors = FALSE) 



```

The Gradient Boosting model improved substantially, with the model correctly predicting `r xgboost_table2[2, 2]` winners and misclassifying `r xgboost_table2[1, 2] + xgboost_table2[2, 1]` observations. This was compared to `r error_count$XGBoost[2]` correctly predicted winners and `r error_count$XGBoost[1]` misclassifications previously. 







### Logit


```{r logit2}

tr_logit2   <- glm(winner ~ ., family = binomial(link="logit"), data = tr3)


tr_logit2   <- suppressWarnings(step(tr_logit2, direction = "backward", trace = FALSE))
tr_logit2 %>% summary


pred_logit2 <- predict(tr_logit2, ts3, type="response")
pred_logit2 <- ifelse(pred_logit2 >= 0.5, 1, 0)

df_logit2   <- data.frame(Actual    = ts3$winner, 
                         Predicted = pred_logit2,
                         stringsAsFactors = FALSE) 


(logit_table2 <- table(Actual    = ts3$winner, 
                      Predicted = pred_logit2))



```

Contrary to the Gradient Boosting model, the Logit model performance significantly decreased, with a total number of misclassifications of `r  logit_table2[1,2] + logit_table2[2,1]` compared to `r error_count$Logit[1]` beforehand. In addition, the total number of correctly predicted winners was `r logit_table2[2, 2]` compared to `r error_count$Logit[2]` previously. 













### Probit

```{r probit2}
tr_probit2   <- glm(winner ~ ., family = binomial(link="probit"), data = tr3)
tr_probit2 %>% summary

tr_probit2   <- suppressWarnings(step(tr_probit2, trace = FALSE))



pred_probit2 <- predict(tr_probit2, ts3, type="response")
pred_probit2 <- ifelse(pred_probit2 >= 0.5, 1, 0)

df_probit2   <- data.frame(Actual    = ts3$winner, 
                          Predicted = pred_probit2,
                          stringsAsFactors = FALSE) 


(probit_table2  <- table(Actual    = ts3$winner, 
                        Predicted = pred_probit2))



```

Similarly to the Logit model, the Probit model has performed significantly worse than previously. It has decreased the number of misclassifications to `r probit_table2[1,2] + probit_table2[2,1]` but the number of correctly predicted winners dropped from `r error_count$Probit[2]` to `r probit_table2[2, 2]`. 






















### All Predictions Combined

Just as I did earlier in the report, I've created a data frame of all predictions side by side called "comparison2". In addition, I created the data frame "error_count2" to show the total misclassifications for each class along with the number of correctly predicted aces, so the methods can be directly compared to one another. 


```{r comparison2}

comparison2 <- data.frame(Actual            = ts3$winner,
                          'Random Forest'   = df_rf2$Predicted,
                          'SVM Radial'      = df_svm_radial2$Predicted,
                          'SVM Linear'      = df_svm_linear2$Predicted,
                          'SVM Polynomial'  = df_svm_polynomial2$Predicted,
                          'Decision Tree'   = df_decisiontree2$Predicted,
                          XGBoost           = df_xgboost2$Predicted,    
                          Logit             = df_logit2$Predicted,
                          Probit            = df_probit2$Predicted, stringsAsFactors = FALSE)




error_count2 <- data.frame('Random Forest'  = c(sum(comparison2$Random.Forest  != comparison2$Actual), rf_table2[2, 2]),
                           'SVM Radial'     = c(sum(comparison2$SVM.Radial     != comparison2$Actual), svm_radial_table2[2, 2]), 
                           'SVM Linear'     = c(sum(comparison2$SVM.Linear     != comparison2$Actual), svm_linear_table2[2, 2]),
                           'SVM Polynomial' = c(sum(comparison2$SVM.Polynomial != comparison2$Actual), svm_polynomial_table2[2, 2]),
                           'Decision Tree'  = c(sum(comparison2$Decision.Tree  != comparison2$Actual), decision_tree_table2[2, 2]),
                           XGBoost          = c(sum(comparison2$XGBoost        != comparison2$Actual), xgboost_table2[2, 2]),
                           Logit            = c(sum(comparison2$Logit          != comparison2$Actual), logit_table2[2, 2]),
                           Probit           = c(sum(comparison2$Probit         != comparison2$Actual), probit_table2[2, 2]), 
                           stringsAsFactors = FALSE)


rownames(error_count2) <- c("Total Misclassifications", "Correct Winner Predictions")



error_count2$Best <- c(ifelse(min(error_count2[1,]) == error_count2$Random.Forest[1],  "Random Forest",
                       ifelse(min(error_count2[1,]) == error_count2$SVM.Radial[1],     "SVM Radial",
                       ifelse(min(error_count2[1,]) == error_count2$SVM.Linear[1],     "SVM Linear",
                       ifelse(min(error_count2[1,]) == error_count2$SVM.Polynomial[1], "SVM Polynomial",
                       ifelse(min(error_count2[1,]) == error_count2$Decision.Tree[1],  "Decision Tree",       
                       ifelse(min(error_count2[1,]) == error_count2$XGBoost[1],        "XGBoost", "Logit")))))),
                    
                       ifelse(max(error_count2[2,]) == error_count2$Random.Forest[2],  "Random Forest",
                       ifelse(max(error_count2[2,]) == error_count2$SVM.Radial[2],     "SVM Radial",
                       ifelse(max(error_count2[2,]) == error_count2$SVM.Linear[2],     "SVM Linear",
                       ifelse(max(error_count2[2,]) == error_count2$SVM.Polynomial[2], "SVM Polynomial",
                       ifelse(max(error_count2[2,]) == error_count2$Decision.Tree[2],  "Decision Tree",
                       ifelse(max(error_count2[2,]) == error_count2$XGBoost[2],        "XGBoost", "Logit")))))))



kable(error_count2, caption = "Model Performance", align = c("c","c","c","c","c","c","c","c","c"))

                         
```

Ultimately, the best performing model in terms of the total number of misclassifications was the SVM Linear model, with 111 misclassifications. However, closer look at the results of this method shows that it didn't actually predict any winners, so all the misclassifications are from the 111 winners in the test sample. 

The best method in terms of number of correctly predicted winners was the XGBoost model, with 44 correctly predicted winners (out of the 111 in the test sample). 









## Comparison of Versions of Models

```{r}

all_comparisons <- rbind(error_count[,-ncol(error_count)], 
                         error_count2[,-ncol(error_count)])


new_row  <- data.frame(Random.Forest  = c(ifelse(all_comparisons$Random.Forest[1] < all_comparisons$Random.Forest[3], "V1", "V2"),
                                          ifelse(all_comparisons$Random.Forest[2] > all_comparisons$Random.Forest[4], "V1", "V2"),
                                          NA),
                       
                       SVM.Radial     = c(ifelse(all_comparisons$SVM.Radial[1] < all_comparisons$SVM.Radial[3], "V1", "V2"),
                                          ifelse(all_comparisons$SVM.Radial[2] > all_comparisons$SVM.Radial[4], "V1", "V2"),
                                          NA),
                       
                       SVM.Linear     = c(ifelse(all_comparisons$SVM.Linear[1] < all_comparisons$SVM.Linear[3], "V1", "V2"),
                                          ifelse(all_comparisons$SVM.Linear[2] > all_comparisons$SVM.Linear[4], "V1", "V2"),
                                          NA),
                       
                      SVM.Polynomial = c(ifelse(all_comparisons$SVM.Polynomial[1] < all_comparisons$SVM.Polynomial[3], "V1", "V2"),
                                         ifelse(all_comparisons$SVM.Polynomial[2] > all_comparisons$SVM.Polynomial[4], "V1", "V2"),
                                         NA),
                                          
                       Decision.Tree  = c(ifelse(all_comparisons$Decision.Tree[1] < all_comparisons$Decision.Tree[3], "V1", "V2"),
                                          ifelse(all_comparisons$Decision.Tree[2] > all_comparisons$Decision.Tree[4], "V1", "V2"),
                                          NA),
                       
                       XGBoost        = c(ifelse(all_comparisons$XGBoost[1] < all_comparisons$XGBoost[3], "V1", "V2"),
                                          ifelse(all_comparisons$XGBoost[2] > all_comparisons$XGBoost[4], "V1", "V2"),
                                          NA),
                                          
                       Logit          = c(ifelse(all_comparisons$Logit[1] < all_comparisons$Logit[3], "V1", "V2"),
                                          ifelse(all_comparisons$Logit[2] > all_comparisons$Logit[4], "V1", "V2"),
                                          NA),
                                          
                       Probit         = c(ifelse(all_comparisons$Probit[1] < all_comparisons$Probit[3], "V1", "V2"),
                                          ifelse(all_comparisons$Probit[2] > all_comparisons$Probit[4], "V1", "V2"),
                                          NA))

all_comparisons <- rbind(all_comparisons, new_row)


rownames(all_comparisons) <- c("Misclassifications Version 1", 
                               "Correct Winner Predictions Version 1",
                               "Misclassifications Version 2",
                               "Correct Winner Predictions Version 2",
                               "Version with Lowest Misclassifications",
                               "Version with Highest Correct Winner Predictions",
                               "Best Version")



# Reclassifying SVM Best Version so there's no ambiguity in the "Best Version" row.
all_comparisons$SVM.Radial[5] <- "V1"





for (i in 1:ncol(all_comparisons)) {
  
  if (all_comparisons[5, i] == all_comparisons[6, i]) {
    
    all_comparisons[7, i] <- all_comparisons[5, i]
    
  } else {
    
    all_comparisons[7, i] <- "Both"
    
  }
  
}



kable(all_comparisons, caption = "All Comparisons", align = rep("c", ncol(all_comparisons)))


```

As we can see from the "all_comparisons" data frame, there are some models that clearly performed better in Version 1 than in Version 2 and vice-versa. There are also some models that had one aspect better in Version 1 but the other aspect being worse than Version 2. 












# Hybriding Models

Using the best version of each model and then hybriding the results, we could take a look at the Parallel Coordinate plot to inspect the reasons why the models can't classify the winners that well. 

```{r}


version1 <- c("SVM.Radial", "SVM.Linear", "SVM.Polynomial", "Logit", "Probit")
version2 <- c("Decision.Tree", "XGBoost", "Random.Forest")


hybrid_comparison   <- cbind(comparison  %>% dplyr::select(Actual, SVM.Radial, SVM.Linear, SVM.Polynomial, Logit, Probit),
                             comparison2 %>% dplyr::select(Decision.Tree, XGBoost, Random.Forest))






for (j in 1:ncol(hybrid_comparison)) {

  hybrid_comparison[, j] <- as.numeric(as.character(hybrid_comparison[, j]))
  
}


hybrid_comparison$Majority <- NA


for (i in 1:nrow(hybrid_comparison)) {
  
  if (sum(hybrid_comparison[i, c(2:(ncol(hybrid_comparison)-1))]) >= 4) {
    
    hybrid_comparison$Majority[i] <- 1
    
  } else {
    
    hybrid_comparison$Majority[i] <- 0
    
  }
  
}



hybrid_comparison$Actual   <- as.factor(hybrid_comparison$Actual)
hybrid_comparison$Majority <- as.factor(hybrid_comparison$Majority)



(hybrid_table <- table(Actual    = hybrid_comparison$Actual,
                       Predicted = hybrid_comparison$Majority))



```

Unfortunately, the hybrid model based off a majority vote hasn't performed too well. In fact, it was outperformed by the first version of the Logit model just by itself. 















