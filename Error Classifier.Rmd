---
title: "Error Classifier"
author: "Dean"
date: "10 July 2017"
output: word_document
---

```{r include=FALSE}
knitr::opts_chunk$set(echo    = FALSE,
                      warning = FALSE, 
                      message = FALSE, 
                      error   = FALSE,
                      cache   = FALSE)

options(scipen = 999)

```




```{r}
load("atp_serves.RData")
df <- atp_serves



library(ggplot2)
library(dplyr)
library(tidyr)
library(gridExtra)
library(GGally)
library(e1071)
library(MASS)
library(randomForest)
library(gbm)
library(rpart)
library(rpart.plot)
library(penalizedLDA)
library(xgboost)
library(foreign)
library(stringr)
library(stringi)
library(devtools)
library(fpp2)
library(neuralnet)



```









## Data Manipulation
### Data Manipulation of Various Column

```{r}


### Removing observation with measurement error ###
df         <- df %>% filter(start.x > 10)
atp_serves <- df %>% filter(start.x > 10)





### Adding Variables and Re-Classifying "serve_classification" Column ###
df$server      <- as.character(df$server)
df$receiver    <- as.character(df$receiver)
df$winner_name <- toupper(df$winner_name)
df$loser_name  <- toupper(df$loser_name)







df$server_hand   <- NA
df$receiver_hand <- NA
df$server_age    <- NA
df$receiver_age  <- NA
df$diff_age      <- NA




for (i in 1:nrow(df)) {
  
  if (df$serve_classification[i] == 0) {
    
    df$serve_classification[i] <- "Ace"
    
  } else {
    
    df$serve_classification[i] <- "Other"
    
  }
  
  
  
  
  
  
  
  
  
  if (atp_serves$serve_classification[i] == 0) {
    
    atp_serves$serve_classification[i] <- "Ace"
    
  } else {
    
    atp_serves$serve_classification[i] <- "Other"
    
  }
  
  
  
  
  
  
  
  
  
  if (str_sub(df$winner_name[i], 
              nchar(df$winner_name[i]) - nchar(df$server[i]) + 1, 
              nchar(df$winner_name[i])) == df$server[i]) {
    
    df$server_hand[i]   <- df$winner_hand[i]
    df$receiver_hand[i] <- df$loser_hand[i]
    df$server_age[i]    <- df$winner_age[i]
    df$receiver_age[i]  <- df$loser_age[i]
    df$diff_age[i]      <- df$server_age[i] - df$receiver_age[i]
    
  } else {
    
    df$server_hand[i]   <- df$loser_hand[i]
    df$receiver_hand[i] <- df$winner_hand[i]
    df$server_age[i]    <- df$loser_age[i]
    df$receiver_age[i]  <- df$winner_age[i]
    df$diff_age[i]      <- df$server_age[i] - df$receiver_age[i]

    
  }
  
  
  
  
  
  
  
  
}






df$serve_classification <- as.factor(df$serve_classification)
df$direction.change     <- as.factor(df$direction.change)
df$round                <- as.factor(df$round)
df$winner_hand          <- as.factor(df$winner_hand)
df$loser_hand           <- as.factor(df$loser_hand)
df$server_hand          <- as.factor(df$server_hand)
df$receiver_hand        <- as.factor(df$receiver_hand)




df$right_to_right_deuce <- ifelse(df$side == "Deuce" & df$server_hand == "R" & df$receiver_hand == "R", 1, 0)
df$right_to_right_ad    <- ifelse(df$side == "Ad" & df$server_hand == "R" & df$receiver_hand == "R", 1, 0)

df$right_to_left_deuce  <- ifelse(df$side == "Deuce" & df$server_hand == "R" & df$receiver_hand == "L", 1, 0)
df$right_to_left_ad     <- ifelse(df$side == "Ad" & df$server_hand == "R" & df$receiver_hand == "L", 1, 0)

df$left_to_right_deuce  <- ifelse(df$side == "Deuce" & df$server_hand == "L" & df$receiver_hand == "R", 1, 0)
df$left_to_right_ad     <- ifelse(df$side == "Ad" & df$server_hand == "L" & df$receiver_hand == "R", 1, 0)

df$left_to_left_deuce   <- ifelse(df$side == "Deuce" & df$server_hand == "L" & df$receiver_hand == "L", 1, 0)
df$left_to_left_ad      <- ifelse(df$side == "Ad" & df$server_hand == "L" & df$receiver_hand == "L", 1, 0)


df                      <- df %>% group_by(side, server) %>% mutate(avg_start.y = mean(start.y)) %>% ungroup()
df$diff_start.y         <- df$start.y - df$avg_start.y


```














### Removing Irrelevant Columns

```{r select_variables}

df <- df %>% dplyr::select(start.x, 
                           start.y, 
                           start.z, 
                           start.1:serve.x, 
                           game:side,
                           serve_classification, 
                           winner_age,
                           loser_age,
                           round,
                           server_age,
                           receiver_age,
                           diff_age,
                           right_to_right_deuce,
                           right_to_right_ad,
                           right_to_left_deuce,
                           right_to_left_ad,
                           diff_start.y,
                           server_hand,
                           receiver_hand)

```
















### Initial Exploration of Data

```{r aces_by_side}

aces_by_side <- atp_serves %>% 
                group_by(side, serve_classification) %>% 
                summarise(n = n()) %>%
                mutate(prop = (100*n) / sum(n))




ggplot(data = aces_by_side) + 
  geom_bar(stat = "identity", aes(x = serve_classification, y = prop, fill = serve_classification)) + 
  facet_wrap(~side, scale = "free_y")



```




```{r aces_by_set}
aces_by_set  <- atp_serves %>%
                group_by(set, serve_classification) %>%
                summarise(n = n()) %>%
                mutate(prop = (100*n) / sum(n))




```





```{r aces_per_side_and_player}
aces_per_side_and_player <- atp_serves %>% 
                            group_by(server, side, serve_classification) %>% 
                            summarise(n = n()) %>% 
                            mutate(proportion = (n * 100)/sum(n))





ggplot(data = rbind(aces_per_side_and_player %>% 
                      filter(serve_classification == "Ace" & side == "Deuce") %>% 
                      arrange(desc(proportion)) %>%
                      head(10),
  
                      aces_per_side_and_player %>% 
                      filter(serve_classification == "Ace" & side == "Ad") %>% 
                      arrange(desc(proportion)) %>%
                      head(10)),
         
         aes(x = reorder(server, proportion), y = proportion, fill = server)) + 
    
  geom_bar(stat = "identity") + 
  geom_text(aes(label = paste(round(proportion, 1), "%", sep ="")), hjust = 1.2, colour = "white") +
  coord_flip() + 
  scale_colour_gradientn(colours = rainbow(4)) +
  xlab("Player") + 
  ylab("Proportion of Aces") + 
  ggtitle("Proportion of Aces Per Side of Court") + 
  theme(legend.position = "none", plot.title = element_text(hjust = 0.5)) +
  facet_wrap(~side, scale = "free")






```






```{r}

ggplot(data = aces_per_side_and_player %>% 
                      filter(serve_classification == "Ace"),
  
         
         aes(x = side, y = proportion, fill = side)) + 
    
  geom_bar(stat = "identity") + 
  scale_colour_gradientn(colours = rainbow(4)) +
  xlab("Player") + 
  ylab("Proportion of Aces") + 
  ggtitle("Proportion of Aces Per Side of Court") + 
  theme(legend.position = "none", plot.title = element_text(hjust = 0.5)) +
  facet_wrap(~server, scale = "free") 


```



```{r aces_per_side_and_hand}
aces_per_side_and_hand <- df %>% 
                            group_by(server_hand, receiver_hand, side, serve_classification) %>% 
                            summarise(n = n()) %>% 
                            mutate(prop = (100*n)/sum(n))

```




```{r individual_players}
individual_players     <- atp_serves %>% 
                            group_by(server, serve_classification) %>% 
                            summarise(n = n()) %>% 
                            mutate(prop = (100*n) / sum(n))




ggplot(data = individual_players) + 
  geom_bar(stat = "identity", aes(x = serve_classification, y = prop, fill = serve_classification)) + 
  facet_wrap(~server, scale = "free_y")




ggplot(data = (individual_players %>% 
                 filter(serve_classification == "Ace") %>% 
                 arrange(desc(prop)) %>% 
                 head(15)), 
        aes(x = reorder(server, prop), y = prop, colour = prop)) + 
        geom_bar(stat = "identity") + 
        coord_flip() + 
        xlab("Player") + 
        ylab("Ace Proportion") +
        geom_text(aes(label = paste(round(prop, 1), "%", sep ="")), hjust = 1.2, colour = "white") +
        ggtitle("Top 15 Ace Proportions") + 
        scale_colour_gradientn(colours = rainbow(4)) +
        theme(legend.position = "none", plot.title = element_text(hjust = 0.5))
```











### Training and Test

```{r training_and_test}

set.seed(1000)
idx <- sample(nrow(df), nrow(df) * (2/3)) 

tr  <- df[idx,]
ts  <- df[-idx,]



ts2 <- atp_serves[-idx,]


tr  <- tr %>% dplyr::select(-server_hand, -receiver_hand, -duration.arc3)
ts  <- ts %>% dplyr::select(-server_hand, -receiver_hand, -duration.arc3)

```








### Random Forest and Variable Selection Based Off Accuracy Measures

```{r random_forest}

tr_rf   <- tr %>% 
            dplyr::select(-serve_classification) %>% 
            randomForest(tr$serve_classification,
                          ntree=2000, 
                          importance = TRUE)


pred_rf <- predict(tr_rf, 
                   ts %>% dplyr::select(-serve_classification), 
                   type="class") #Prediction using test data

df_rf <- data.frame(Actual = ts$serve_classification, 
                    Predicted = pred_rf,
                    stringsAsFactors = FALSE)

table(Actual = ts$serve_classification, Predicted = pred_rf)



rf_imp <- data.frame(Var = rownames(tr_rf$importance), tr_rf$importance, stringsAsFactors = FALSE) %>%
                     arrange(desc(MeanDecreaseAccuracy))

varImpPlot(tr_rf, sort = TRUE)








### Filtering the Columns Based Off Importance ###

rf_imp$dummy <- ifelse(rf_imp$MeanDecreaseGini > 2.8 | rf_imp$MeanDecreaseAccuracy > 0.001, 1, 0)
rf_imp       <- rf_imp[which(rf_imp$dummy == 1),]
rf_var_list  <- rf_imp$Var
rf_var_list[length(rf_var_list) + 1] <- "serve_classification"




tr <- tr[, which(colnames(tr) %in% rf_var_list)]
ts <- ts[, which(colnames(ts) %in% rf_var_list)]



```










### Random Forest With Most Important Variables

```{r random_forest2}

tr_rf   <- tr %>% 
            dplyr::select(-serve_classification) %>% 
            randomForest(tr$serve_classification,
                         ntree=2000, 
                         importance = TRUE)


pred_rf <- predict(tr_rf, 
                   ts %>% dplyr::select(-serve_classification), 
                   type="class") #Prediction using test data


df_rf <- data.frame(Actual = ts$serve_classification, 
                    Predicted = pred_rf,
                    stringsAsFactors = FALSE)


(rf_table <- table(Actual = ts$serve_classification, Predicted = pred_rf))



rf_imp <- data.frame(Var = rownames(tr_rf$importance), tr_rf$importance, stringsAsFactors = FALSE) %>%
          arrange(desc(MeanDecreaseAccuracy))


varImpPlot(tr_rf, sort = TRUE)

```







## SVM

### SVM Radial

```{r svm_radial}

tr_svm_radial   <- svm(serve_classification ~ ., 
                      data = tr, 
                      kernel = "radial")


pred_svm_radial <- predict(tr_svm_radial, ts)          
df_svm_radial   <- data.frame(Actual = ts$serve_classification, 
                              Predicted = pred_svm_radial) 


(svm_radial_table <- table(ts$serve_classification, pred_svm_radial))

```











### SVM Linear

```{r svm_linear}

tr_svm_linear   <- svm(serve_classification ~ ., 
                      data = tr, 
                      kernel = "linear")


pred_svm_linear <- predict(tr_svm_linear, ts)          
df_svm_linear   <- data.frame(Actual = ts$serve_classification, 
                              Predicted = pred_svm_linear) 


(svm_linear_table <- table(ts$serve_classification, pred_svm_linear))

```












### SVM Polynomial

```{r svm_polynomial}

tr_svm_polynomial   <- svm(serve_classification ~ ., 
                      data = tr, 
                      kernel = "polynomial")


pred_svm_polynomial <- predict(tr_svm_polynomial, ts)          
df_svm_polynomial   <- data.frame(Actual = ts$serve_classification, 
                                  Predicted = pred_svm_polynomial) 


(svm_polynomial_table <- table(ts$serve_classification, pred_svm_polynomial))

```




















### Decision Tree 

```{r decision_tree}

tr_decisiontree <- rpart(serve_classification ~ ., 
                         data=tr,
                         method="class")         

prp(tr_decisiontree)     



pred_decisiontree <- predict(tr_decisiontree,              
                             ts, 
                             type="class")


df_decisiontree   <- data.frame(Actual    = ts$serve_classification, 
                                Predicted = pred_decisiontree,
                                stringsAsFactors = FALSE) 


(decision_tree_table <- table(Actual    = ts$serve_classification, 
                              Predicted = pred_decisiontree))


```








### LDA

```{r LDA}
# lda_sub_df <- tr %>% select()
#
# num_of_aces <- tr %>% group_by(serve_classification) %>% summarise(n = n()) %>% mutate(prop = n/sum(n))
#
# lda_model <- lda(serve_classification ~ .,                    
#                  data = tr,    
#                  prior = c(num_of_aces$prop[1], num_of_aces$prop[2]),         
#                  kernel="radial")
# 
# pred_lda <- predict(lda_model, ts, type="class")
```












### Penalized LDA

```{r penalizedLDA}

cls <- ifelse(tr[,"serve_classification"] == "Ace", 2, 1)
# set.seed(1)
# tr_plda <- PenalizedLDA(as.matrix(tr[,-c(21)]), cls, 
#                         as.matrix(ts[,-c(21)]), lambda=0.001, K=1)
# 
# 
# 
# table(Actual = ts$serve_classification, tr_plda$ypred)

```






### XGBoost

```{r xgboost}


cls <- ifelse(tr[,"serve_classification"] == "Ace", 2, 1)

dtrain <- tr %>% 
          dplyr::select(-serve_classification) %>% 
          as.matrix %>% 
          xgb.DMatrix(label=cls)

param <- list(max.depth = 2, eta = 1, silent = 1)
tr_xgb <- xgb.train(param, dtrain, nthread = 2, nround = 10)


ts_matrix <- ts %>% 
              dplyr::select(-serve_classification) %>% 
              as.matrix

pxgb <- round(predict(tr_xgb, ts_matrix), 0)

pxgb <- ifelse(pxgb == 2, "Ace", "Other")
pxgb <- as.factor(pxgb)




(xgboost_table <- table(ts$serve_classification, pxgb))

df_xgboost   <- data.frame(Actual    = ts$serve_classification, 
                           Predicted = pxgb,
                           stringsAsFactors = FALSE) 



```









### Logit

```{r logit}
tr_logit   <- glm(serve_classification ~ ., family = binomial(link="logit"), data = tr)
tr_logit %>% summary

tr_logit   <- suppressWarnings(step(tr_logit))



pred_logit <- predict(tr_logit, ts, type="response")
pred_logit <- ifelse(pred_logit >= 0.5, "Other", "Ace")

df_logit   <- data.frame(Actual    = ts$serve_classification, 
                         Predicted = pred_logit,
                         stringsAsFactors = FALSE) 


(logit_table <- table(Actual    = ts$serve_classification, 
                      Predicted = pred_logit))



```














### Probit

```{r probit}
tr_probit   <- glm(serve_classification ~ ., family = binomial(link="probit"), data = tr)
tr_probit %>% summary

tr_probit   <- suppressWarnings(step(tr_probit))



pred_probit <- predict(tr_probit, ts, type="response")
pred_probit <- ifelse(pred_probit >= 0.5, "Other", "Ace")

df_probit   <- data.frame(Actual    = ts$serve_classification, 
                          Predicted = pred_probit,
                          stringsAsFactors = FALSE) 


(probit_table  <- table(Actual    = ts$serve_classification, 
                        Predicted = pred_probit))



```










### Neural Networks

```{r neural_networks}

# neuralnet_df       <- tr
# neuralnet_df$dummy <- ifelse(neuralnet_df$serve_classification == "Ace", 1, 0)
# neuralnet_df       <- neuralnet_df %>% dplyr::select(-serve_classification)
# 
# equation <- as.formula(paste('dummy ~ ' , paste(colnames(neuralnet_df)[1:(length(colnames(neuralnet_df))-1)], collapse='+')))
# 
# 
# tr_neuralnet <- neuralnet(equation, data = neuralnet_df, hidden = 1)


```



















### All Predictions Combined

```{r comparison}
comparison <- data.frame(Actual            = ts$serve_classification,
                         'Random Forest'   = df_rf$Predicted,
                         'SVM Radial'      = df_svm_radial$Predicted,
                         'SVM Linear'      = df_svm_linear$Predicted,
                         'SVM Polynomial'  = df_svm_polynomial$Predicted,
                         'Decision Tree'   = df_decisiontree$Predicted,
                         XGBoost           = df_xgboost$Predicted,
                         Logit             = df_logit$Predicted,
                         Probit            = df_probit$Predicted)



error_count <- data.frame('Random Forest'  = c(sum(comparison$Random.Forest  != comparison$Actual), rf_table[1, 1]),
                          'SVM Radial'     = c(sum(comparison$SVM.Radial     != comparison$Actual), svm_radial_table[1, 1]), 
                          'SVM Linear'     = c(sum(comparison$SVM.Linear     != comparison$Actual), svm_linear_table[1, 1]),
                          'SVM Polynomial' = c(sum(comparison$SVM.Polynomial != comparison$Actual), svm_polynomial_table[1, 1]),
                          'Decision Tree'  = c(sum(comparison$Decision.Tree  != comparison$Actual), decision_tree_table[1, 1]),
                          XGBoost          = c(sum(comparison$XGBoost        != comparison$Actual), xgboost_table[1, 1]),
                          Logit            = c(sum(comparison$Logit          != comparison$Actual), logit_table[1, 1]),
                          Probit           = c(sum(comparison$Probit         != comparison$Actual), probit_table[1, 1]))

rownames(error_count) <- c("Total Misclassifications", "Correct Ace Prediction")



error_count$Best <- c(ifelse(min(error_count[1,]) == error_count$Random.Forest[1],  "Random Forest",
                      ifelse(min(error_count[1,]) == error_count$SVM.Radial[1],     "SVM Radial",
                      ifelse(min(error_count[1,]) == error_count$SVM.Linear[1],     "SVM Linear",
                      ifelse(min(error_count[1,]) == error_count$SVM.Polynomial[1], "SVM Polynomial",
                      ifelse(min(error_count[1,]) == error_count$Decision.Tree[1],  "Decision Tree",
                      ifelse(min(error_count[1,]) == error_count$XGBoost[1],        "XGBoost", "Logit")))))),
                    
                      ifelse(max(error_count[2,]) == error_count$Random.Forest[1],  "Random Forest",
                      ifelse(max(error_count[2,]) == error_count$SVM.Radial[1],     "SVM Radial",
                      ifelse(max(error_count[2,]) == error_count$SVM.Linear[1],     "SVM Linear",
                      ifelse(max(error_count[2,]) == error_count$SVM.Polynomial[1], "SVM Polynomial",
                      ifelse(max(error_count[2,]) == error_count$Decision.Tree[1],  "Decision Tree",
                      ifelse(max(error_count[2,]) == error_count$XGBoost[1],        "XGBoost", "Logit")))))))


                         
```

Ultimately, given the variables chosen, the best classification method in terms of misclassification rate is the `r error_count$Best[1]` model with `r min(error_count[1, 1:(ncol(error_count)-1)])` misclassifications. The best model in terms of the number of correctly predicted aces is the `r error_count$Best[2]` model with `r max(error_count[2, 1:(ncol(error_count)-1)])` correctly predicted aces.   













### Hybrid Classification Using Majority Voting

```{r}

weights <- c(sum(error_count[1, 1:(ncol(error_count) - 1)]) / ((ncol(error_count)-1) * error_count[1, 1:(ncol(error_count) - 1)]))

weights <- unlist(weights)




hybrid_comparison <- comparison


for (j in 1:ncol(hybrid_comparison)) {
  
  hybrid_comparison[, j] <- as.character(hybrid_comparison[, j])
  hybrid_comparison[, j] <- ifelse(hybrid_comparison[, j] == "Other", 0, 1)
 
  
}




hybrid_comparison$weighted_pred <- ifelse(hybrid_comparison$Random.Forest  * weights["Random.Forest"] + 
                                          hybrid_comparison$SVM.Radial     * weights["SVM.Radial"] + 
                                          hybrid_comparison$SVM.Linear     * weights["SVM.Linear"] + 
                                          hybrid_comparison$SVM.Polynomial * weights["SVM.Polynomial"] + 
                                          hybrid_comparison$Decision.Tree  * weights["Decision.Tree"] + 
                                          hybrid_comparison$XGBoost        * weights["XGBoost"] + 
                                          hybrid_comparison$Logit          * weights["Logit"] + 
                                          hybrid_comparison$Probit         * weights["Probit"] > (ncol(comparison) - 1) / 2,
                                           
                                          "Ace", "Other")



for (j in 1:(ncol(hybrid_comparison)-1)) {
  
  hybrid_comparison[, j] <- ifelse(hybrid_comparison[, j] == 0, "Other", "Ace")
  hybrid_comparison[, j] <- as.character(hybrid_comparison[, j])
  
}


table(Actual    = hybrid_comparison$Actual, 
      Predicted = hybrid_comparison$weighted_pred)


```















### Boxplot of Data

```{r}

df_long1 <- gather(data = df, 
                   key = variables, 
                   value = stat, 
                   start.y, 
                   start.z, 
                   start.1:angle.change, 
                   center.x:point, 
                   winner_age,
                   loser_age,
                   -direction.change)
       
                
ggplot(df_long1) + 
  geom_boxplot(aes(x = serve_classification, y = stat, fill = serve_classification)) + 
  facet_wrap(~variables, scale = "free_y")




df_long2 <- gather(data = df, 
                   key = variables,
                   value = stat,
                   speed:height_off_bounce,
                   angle.change:duration.arc1, 
                   set,
                   diff_age,
                   diff_start.y)


ggplot(df_long2) + 
  geom_boxplot(aes(x = serve_classification, y = stat, fill = serve_classification)) + 
  facet_wrap(~variables, scale = "free_y")



ggplot(data = df) + 
  geom_density(aes(x = z0.1, group = serve_classification, colour = serve_classification))



```










## Federer Match Data

```{r}
# setwd("~/2017 Work/intentions/intentions/data")
# load("~/2017 Work/intentions/intentions/data/federer2017.RData")


```

